{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd955366ad0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1013)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/codegen.csv\")\n",
    "data_x = df[\"utterance\"]\n",
    "data_y = df[\"targets\"]\n",
    "\n",
    "\"\"\"\n",
    "# import dataset\n",
    "with open(\"calculator.dataset\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "data_x, data_y = [], []\n",
    "for line in lines:\n",
    "    if (line[0] == \"(\"):\n",
    "        data_y.append(line.strip())\n",
    "    elif (line != \"\\n\"):\n",
    "        data_x.append(line.strip())\n",
    "\"\"\"\n",
    "# split into test/train data\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size=0.2)\n",
    "\n",
    "test_x.loc[38] = \"What is the minimum humidity?\"\n",
    "test_y.loc[38] = \"min( WeatherHistory [ 'Humidity' ] )\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building input and output vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NULL': 0, 'UNK': 1, '<end>': 2, 'Can': 3, 'you': 4, 'give': 5, 'the': 6, 'correlation': 7, 'value': 8, 'between': 9, 'variables': 10, 'temperature': 11, 'and': 12, 'feeling_temperature?': 13, 'predict': 14, 'from': 15, 'model': 16, 'what': 17, 'humidity': 18, 'is': 19, '12?': 20, 'What': 21, 'average': 22, 'temperature?': 23, 'would': 24, 'be': 25, 'a': 26, 'of': 27, 'Give': 28, 'me': 29, 'highest': 30, 'how': 31, 'does': 32, 'affect': 33, 'humidity?': 34, 'Find': 35, 'linear': 36, 'at': 37, '12': 38, 'degrees.': 39, 'when': 40, 'it': 41, 'degrees': 42, 'will': 43, 'be?': 44, '12.': 45, 'Describe': 46, 'by': 47, 'using': 48, 'hot': 49, 'get?': 50, 'Create': 51, 'in': 52, 'Weather': 53, 'History': 54, 'data': 55, 'Tell': 56, 'minimum': 57, 'is.': 58, \"What's\": 59, 'predicted': 60, 'cold': 61, 'mean': 62, 'relationship': 63, 'maximum': 64, 'dataset?': 65, 'What’s': 66, 'coldest': 67, 'today?': 68, 'extent': 69, 'are': 70, 'Of': 71, 'listed': 72, 'values,': 73, 'which': 74, 'feeling': 75, 'temperature.': 76, 'hottest': 77, 'At': 78, 'degrees,': 79, \"what's\": 80, 'on': 81, 'any': 82, 'day?': 83, 'to': 84, 'How': 85, 'celsius?': 86, 'value.': 87, 'Predict': 88, 'temperatures': 89, 'WeatherHistory?': 90, 'based': 91, '_': 92, '_?': 93, 'regression': 94, 'find': 95, 'value?': 96, 'Do': 97, 'values': 98, 'for': 99, 'described': 100, 'previous': 101, 'your': 102, 'prediction': 103, 'tell': 104, 'all': 105, 'weather': 106, \"it's\": 107, 'ever': 108, 'lowest': 109, 'recorded': 110, 'WeatherHistory.': 111, 'Calculate': 112, 'recorded?': 113, 'listed?': 114, 'Is': 115, 'actual': 116, 'was': 117, 'predicting': 118, 'like': 119, 'outside,': 120, 'change': 121, 'there': 122, 'β̂': 123, 'question.': 124, 'history?': 125}\n",
      "{'NULL': 0, 'UNK': 1, '<end>': 2, 'cor(': 3, 'WeatherHistroy': 4, '[': 5, \"'Temperature'\": 6, ']': 7, ',': 8, 'WeatherHistory': 9, \"'FellingTemperature'\": 10, ')': 11, 'predict(': 12, 'mod': 13, '12': 14, 'mean(': 15, 'max(': 16, 'lm(': 17, \"'Humidity'\": 18, 'min(': 19}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class Vocabulary():\n",
    "    END_OF_SENTENCE = '<end>'\n",
    "    NULL = 'NULL'\n",
    "    UNKNOWN = 'UNK'\n",
    "    END_OF_SENTENCE_INDEX = 2\n",
    "    def __init__(self):\n",
    "        self.tok2ind = {self.NULL: 0, self.UNKNOWN: 1, self.END_OF_SENTENCE: 2}\n",
    "        self.ind2tok = {0: self.NULL, 1: self.UNKNOWN, 2: self.END_OF_SENTENCE}\n",
    "    \n",
    "    def add(self, token):\n",
    "        if token not in self.tok2ind:\n",
    "            index = len(self.tok2ind)\n",
    "            self.tok2ind[token] = index\n",
    "            self.ind2tok[index] = token\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tok2ind)\n",
    "    \n",
    "    def get_index(self, word):\n",
    "        if word in self.tok2ind:\n",
    "            return self.tok2ind[word]\n",
    "        return self.tok2ind[self.UNKNOWN]\n",
    "    \n",
    "    def get_word(self, i):\n",
    "        return self.ind2tok[i]\n",
    "\n",
    "    def sentence_to_indices(self, sentence):\n",
    "        words = [x for x in sentence.split(' ')]\n",
    "        words.append(self.END_OF_SENTENCE)\n",
    "        indices = [self.get_index(w) for w in words]\n",
    "        return indices\n",
    "\n",
    "def build_vocab(examples):\n",
    "    counts = Counter()\n",
    "    for ex in examples:\n",
    "        words = [w for w in ex.split(' ') if w.strip()]\n",
    "        counts.update(words)\n",
    "    \n",
    "    word_list = [w for w in counts if counts[w] > 1]\n",
    "    \n",
    "    word_dict = Vocabulary()\n",
    "    for w in word_list:\n",
    "        word_dict.add(w)\n",
    "    return word_dict\n",
    "\n",
    "input_vocab = build_vocab(train_x)\n",
    "output_vocab = build_vocab(train_y)\n",
    "print(input_vocab.tok2ind)\n",
    "print(output_vocab.tok2ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 6x9]\n",
      "\n",
      "\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      "[torch.FloatTensor of size 6x6]\n",
      "\n",
      "\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      "[torch.FloatTensor of size 6x6]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 11x8]\n",
      "\n",
      "\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      "[torch.FloatTensor of size 6x6]\n",
      "\n",
      "\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      "[torch.FloatTensor of size 6x5]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 11x8]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0\n",
      "    0     0     0     0     0     0\n",
      "    0     0     0     0     0     0\n",
      "    0     0     0     0     0     0\n",
      "    0     0     0     0     0     0\n",
      "    0     0     0     0     0     0\n",
      "    0     0     0     0     0     0\n",
      "    0     0     0     0     0     0\n",
      "    0     0     0     0     0     0\n",
      "    0     0     0     0     0     0\n",
      "    0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 11x6]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 11x7]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 6x12]\n",
      "\n",
      "\n",
      " 0  0  0  0\n",
      " 0  0  0  0\n",
      " 0  0  0  0\n",
      " 0  0  0  0\n",
      " 0  0  0  0\n",
      " 0  0  0  0\n",
      "[torch.FloatTensor of size 6x4]\n",
      "\n",
      "\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      "[torch.FloatTensor of size 6x6]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     1     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 5x13]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 11x8]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 11x9]\n",
      "\n",
      "\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "[torch.FloatTensor of size 11x5]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 6x7]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 11x7]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 11x8]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 11x10]\n",
      "\n",
      "\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      "[torch.FloatTensor of size 6x6]\n",
      "\n",
      "\n",
      " 0  0  0  0\n",
      " 0  0  0  0\n",
      " 0  0  0  0\n",
      " 0  0  0  0\n",
      " 0  0  0  0\n",
      " 0  0  0  0\n",
      "[torch.FloatTensor of size 6x4]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 5x12]\n",
      "\n",
      "\n",
      "\n",
      "Columns 0 to 12 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 13 to 13 \n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "[torch.FloatTensor of size 11x14]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 6x8]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 6x7]\n",
      "\n",
      "\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      "[torch.FloatTensor of size 6x6]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 6x11]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 11x9]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 11x8]\n",
      "\n",
      "\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      "[torch.FloatTensor of size 6x5]\n",
      "\n",
      "\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      "[torch.FloatTensor of size 6x6]\n",
      "\n",
      "\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "[torch.FloatTensor of size 11x5]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 11x10]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 6x8]\n",
      "\n",
      "\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      "[torch.FloatTensor of size 6x5]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 6x11]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 6x9]\n",
      "\n",
      "\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      "[torch.FloatTensor of size 6x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import Sampler\n",
    "\n",
    "class Example():\n",
    "    def __init__(self, x_str, y_str, input_vocab, output_vocab):\n",
    "        self.x_str = x_str\n",
    "        self.y_str = y_str\n",
    "        self.x_toks = x_str.split(' ')\n",
    "        self.y_toks = y_str.split(' ')\n",
    "        \n",
    "        self.input_vocab = input_vocab\n",
    "        self.output_vocab = output_vocab\n",
    "        self.x_inds = torch.LongTensor(input_vocab.sentence_to_indices(x_str))\n",
    "        self.y_inds = torch.LongTensor(output_vocab.sentence_to_indices(y_str))\n",
    "        \n",
    "        # for copying\n",
    "        self.y_in_x_inds = torch.FloatTensor(([[int(x_tok == y_tok) for x_tok in self.x_toks] for y_tok in self.y_toks])) \n",
    "\n",
    "# In order to use PyTorch's data loader\n",
    "class ReaderDataset(Dataset):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.examples[index]\n",
    "    \n",
    "train_exs = []\n",
    "for x,y in zip(train_x, train_y):\n",
    "    train_exs.append(Example(x, y, input_vocab, output_vocab))\n",
    "train_dataset = ReaderDataset(train_exs)\n",
    "\n",
    "test_exs = []\n",
    "for x,y in zip(test_x, test_y):\n",
    "    test_exs.append(Example(x, y, input_vocab, output_vocab))\n",
    "test_dataset = ReaderDataset(test_exs)\n",
    "\n",
    "for x in test_dataset:\n",
    "    print(x.y_in_x_inds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize individual examples and organize them into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\n",
      "   21    19     6  ...      0     0     0\n",
      "   56    29     6  ...      0     0     0\n",
      "   56    29     6  ...      0     0     0\n",
      "       ...          ⋱          ...       \n",
      "   56    29     6  ...      0     0     0\n",
      "   21    24     6  ...      0     0     0\n",
      "   21     1     4  ...      0     0     0\n",
      "[torch.LongTensor of size 100x22]\n",
      ", \n",
      "   11\n",
      "   29\n",
      "   55\n",
      "   71\n",
      "  100\n",
      "  119\n",
      "  141\n",
      "  158\n",
      "  187\n",
      "  207\n",
      "  229\n",
      "  248\n",
      "  273\n",
      "  295\n",
      "  318\n",
      "  335\n",
      "  359\n",
      "  380\n",
      "  417\n",
      "  426\n",
      "  451\n",
      "  475\n",
      "  492\n",
      "  512\n",
      "  534\n",
      "  556\n",
      "  581\n",
      "  600\n",
      "  628\n",
      "  644\n",
      "  667\n",
      "  688\n",
      "  716\n",
      "  732\n",
      "  759\n",
      "  774\n",
      "  798\n",
      "  819\n",
      "  848\n",
      "  866\n",
      "  890\n",
      "  907\n",
      "  928\n",
      "  954\n",
      "  979\n",
      "  998\n",
      " 1020\n",
      " 1040\n",
      " 1066\n",
      " 1086\n",
      " 1110\n",
      " 1136\n",
      " 1154\n",
      " 1171\n",
      " 1193\n",
      " 1217\n",
      " 1240\n",
      " 1264\n",
      " 1286\n",
      " 1310\n",
      " 1325\n",
      " 1347\n",
      " 1372\n",
      " 1399\n",
      " 1413\n",
      " 1437\n",
      " 1461\n",
      " 1492\n",
      " 1507\n",
      " 1524\n",
      " 1552\n",
      " 1570\n",
      " 1594\n",
      " 1618\n",
      " 1634\n",
      " 1660\n",
      " 1681\n",
      " 1701\n",
      " 1722\n",
      " 1748\n",
      " 1766\n",
      " 1787\n",
      " 1811\n",
      " 1836\n",
      " 1854\n",
      " 1875\n",
      " 1899\n",
      " 1921\n",
      " 1942\n",
      " 1967\n",
      " 1984\n",
      " 2012\n",
      " 2033\n",
      " 2056\n",
      " 2075\n",
      " 2099\n",
      " 2122\n",
      " 2141\n",
      " 2166\n",
      " 2191\n",
      "[torch.LongTensor of size 100]\n",
      ", \n",
      "    0     0     0  ...      1     1     1\n",
      "    0     0     0  ...      1     1     1\n",
      "    0     0     0  ...      1     1     1\n",
      "       ...          ⋱          ...       \n",
      "    0     0     0  ...      1     1     1\n",
      "    0     0     0  ...      1     1     1\n",
      "    0     0     0  ...      1     1     1\n",
      "[torch.ByteTensor of size 100x22]\n",
      ", \n",
      "   15     9     5  ...      0     0     0\n",
      "   19     9     5  ...      0     0     0\n",
      "    3     4     5  ...      7    11     2\n",
      "       ...          ⋱          ...       \n",
      "   16     9     5  ...      0     0     0\n",
      "   12    13     8  ...      0     0     0\n",
      "   12    13     8  ...      0     0     0\n",
      "[torch.LongTensor of size 100x12]\n",
      ", \n",
      "    1     1     1  ...      0     0     0\n",
      "    1     1     1  ...      0     0     0\n",
      "    1     1     1  ...      1     1     1\n",
      "       ...          ⋱          ...       \n",
      "    1     1     1  ...      0     0     0\n",
      "    1     1     1  ...      0     0     0\n",
      "    1     1     1  ...      0     0     0\n",
      "[torch.ByteTensor of size 100x12]\n",
      ", \n",
      "( 0 ,.,.) = \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "     ...       ⋱       ...    \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "\n",
      "( 1 ,.,.) = \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "     ...       ⋱       ...    \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "\n",
      "( 2 ,.,.) = \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "     ...       ⋱       ...    \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "... \n",
      "\n",
      "(97 ,.,.) = \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "     ...       ⋱       ...    \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "\n",
      "(98 ,.,.) = \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "     ...       ⋱       ...    \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "\n",
      "(99 ,.,.) = \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "     ...       ⋱       ...    \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "[torch.FloatTensor of size 100x12x22]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# vectorize batch data\n",
    "def vectorize(batch):\n",
    "    max_input_length = max([ex.x_inds.size(0) for ex in batch])\n",
    "    x = torch.LongTensor(len(batch), max_input_length).zero_() # initialize to 0\n",
    "    x_mask = torch.ByteTensor(len(batch), max_input_length).fill_(1) # mask used in softmax\n",
    "    x_lens = torch.LongTensor(len(batch)).zero_()\n",
    "    for i, ex in enumerate(batch):\n",
    "        x[i, :ex.x_inds.size(0)].copy_(ex.x_inds)\n",
    "        x_mask[i, :ex.x_inds.size(0)].fill_(0)\n",
    "        ###CHANGE: x_lens store the last index of each sequence. i*max_input_length is added so that later we can use \n",
    "        ###torch.index_select to get the last hidden states from a 2D tensor (batch_size*max_input_length, embedding_dim)\n",
    "        x_lens[i] = i*max_input_length+ex.x_inds.size(0)-1 \n",
    "    \n",
    "    max_output_length = max([ex.y_inds.size(0) for ex in batch])\n",
    "    y = torch.LongTensor(len(batch), max_output_length).zero_()\n",
    "    y_mask = torch.ByteTensor(len(batch), max_output_length).zero_() # for masked_select\n",
    "    for i, ex in enumerate(batch):\n",
    "        y[i, :ex.y_inds.size(0)].copy_(ex.y_inds)\n",
    "        y_mask[i, :ex.y_inds.size(0)].fill_(1)\n",
    "    \n",
    "    # for copying\n",
    "    y_in_x_inds = torch.FloatTensor(len(batch), max_output_length, max_input_length).zero_()\n",
    "    for i, ex in enumerate(batch):\n",
    "        y_in_x_inds[i, :ex.y_in_x_inds.size(0), :ex.y_in_x_inds.size(1)].copy_(ex.y_in_x_inds)\n",
    "\n",
    "    return x, x_lens, x_mask, y, y_mask, y_in_x_inds\n",
    "\n",
    "train_sampler = torch.utils.data.sampler.RandomSampler(train_dataset)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=100, ## the batch_size can be tuned\n",
    "    sampler=train_sampler,\n",
    "    num_workers=1,\n",
    "    collate_fn=vectorize\n",
    ")\n",
    "\n",
    "test_sampler = torch.utils.data.sampler.SequentialSampler(test_dataset)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1, ## the batch_size can be tuned\n",
    "    sampler=test_sampler,\n",
    "    num_workers=1,\n",
    "    collate_fn=vectorize\n",
    ")\n",
    "\n",
    "for x in train_loader:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Build the seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack bidirectional LSTM\n",
    "class StackBRNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=1):\n",
    "        super(StackBRNN, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.rnns = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            input_dim = input_dim if i == 0 else hidden_dim * 2\n",
    "            self.rnns.append(nn.LSTM(input_dim, hidden_dim, bidirectional=True))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Transpose batch and sequence dims\n",
    "        x = x.transpose(0, 1) # (seq_len, batch_size, input_dim)\n",
    "\n",
    "        outputs = [x]\n",
    "        for i in range(self.num_layers):\n",
    "            rnn_input = outputs[-1]\n",
    "            rnn_output = self.rnns[i](rnn_input)[0]\n",
    "            outputs.append(rnn_output)\n",
    "\n",
    "        h_output = outputs[-1]\n",
    "\n",
    "        # Transpose back\n",
    "        h_output = h_output.transpose(0, 1) # (batch_size, seq_len, 2*hidden_dim)\n",
    "        \n",
    "        return h_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2.1: Define the basic seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, input_vocab, output_vocab, copying=False):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.input_vocab = input_vocab\n",
    "        self.output_vocab = output_vocab\n",
    "        self.in_vocab_size = len(self.input_vocab)\n",
    "        self.out_vocab_size = len(self.output_vocab)\n",
    "        \n",
    "        self.in_embedding = nn.Embedding(self.in_vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.encoder = StackBRNN(embedding_dim, hidden_dim)\n",
    "        \n",
    "        self.out_embedding = nn.Embedding(self.out_vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        #Inputs: input, (h_0, c_0)\n",
    "        #Outputs: h_1, c_1\n",
    "        self.decoder = nn.LSTMCell(embedding_dim, hidden_dim) \n",
    "         \n",
    "        self.enc_to_dec = nn.Linear(hidden_dim*2, hidden_dim) # project encoding outupt\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_dim, self.out_vocab_size)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x_emb = self.in_embedding(x)\n",
    "        output = self.encoder(x_emb) # output: (batch_size, seq_len, hidden_dim*2)\n",
    "        return output\n",
    "    \n",
    "    def decode(self, h_prev):\n",
    "        out = self.output_layer(h_prev[0])\n",
    "        probs = F.softmax(out, dim=1)\n",
    "        \n",
    "        return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attention.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSeq2Seq(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, input_vocab, output_vocab, copying=False):\n",
    "        super(AttentionSeq2Seq, self).__init__()\n",
    "        self.input_vocab = input_vocab\n",
    "        self.output_vocab = output_vocab\n",
    "        self.in_vocab_size = len(self.input_vocab)\n",
    "        self.out_vocab_size = len(self.output_vocab)\n",
    "        self.copying = copying\n",
    "        \n",
    "        self.in_embedding = nn.Embedding(self.in_vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.encoder = StackBRNN(embedding_dim, hidden_dim)\n",
    "        \n",
    "        self.out_embedding = nn.Embedding(self.out_vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        #Inputs: input, (h_0, c_0)\n",
    "        #Outputs: h_1, c_1\n",
    "        self.decoder = nn.LSTMCell(embedding_dim + hidden_dim*2, hidden_dim) # concatenate y_t and context_t\n",
    "        \n",
    "        self.enc_to_dec = nn.Linear(hidden_dim*2, hidden_dim) # project encoding outupt\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_dim + hidden_dim*2, self.out_vocab_size) # concatenate h_t and context_t\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x_emb = self.in_embedding(x)\n",
    "        output = self.encoder(x_emb) # output: (batch_size, seq_len, hidden_dim*2)\n",
    "        return output\n",
    "    \n",
    "    def decode(self, encoder_outputs, encoder_proj_outputs, x_mask, h_prev):\n",
    "        # (batch_size, seq_len, hidden_dim) * (batch_size, hidden_dim, 1) - >(batch_size, seq_len, 1)\n",
    "        scores = torch.bmm(encoder_proj_outputs, h_prev[0].unsqueeze(2)).squeeze(2) # scores: (batch_size, seq_len)\n",
    "        scores.data.masked_fill_(x_mask.data, -float('inf'))\n",
    "        alpha = F.softmax(scores, dim=1)\n",
    "        # (batch_size, 1, seq_len) * (batch_size, seq_len, hidden_dim) - > (batch_size, 1, hidden_dim)\n",
    "        context_t = torch.bmm(alpha.unsqueeze(1), encoder_outputs).squeeze(1) # context_t: (batch_size, hidden_dim) \n",
    "        \n",
    "        out = self.output_layer(torch.cat([h_prev[0], context_t], 1))\n",
    "        \n",
    "        if self.copying: \n",
    "            probs = F.softmax(torch.cat([out, scores], 1), dim=1) # Appending scores over the input\n",
    "        else:\n",
    "            probs = F.softmax(out, dim=1)\n",
    "    \n",
    "        return probs, context_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2.2: Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize and train the network:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch = 0 | Loss = 56.24\n",
      "Epoch = 1 | Loss = 55.67\n",
      "Epoch = 2 | Loss = 55.21\n",
      "Epoch = 3 | Loss = 55.55\n",
      "Epoch = 4 | Loss = 54.73\n",
      "Epoch = 5 | Loss = 53.14\n",
      "Epoch = 6 | Loss = 52.47\n",
      "Epoch = 7 | Loss = 52.00\n",
      "Epoch = 8 | Loss = 51.60\n",
      "Epoch = 9 | Loss = 50.18\n",
      "Epoch = 10 | Loss = 49.51\n",
      "Epoch = 11 | Loss = 48.55\n",
      "Epoch = 12 | Loss = 47.49\n",
      "Epoch = 13 | Loss = 47.14\n",
      "Epoch = 14 | Loss = 46.08\n",
      "Epoch = 15 | Loss = 45.08\n",
      "Epoch = 16 | Loss = 44.54\n",
      "Epoch = 17 | Loss = 43.42\n",
      "Epoch = 18 | Loss = 41.79\n",
      "Epoch = 19 | Loss = 41.97\n",
      "Epoch = 20 | Loss = 40.65\n",
      "Epoch = 21 | Loss = 39.91\n",
      "Epoch = 22 | Loss = 38.98\n",
      "Epoch = 23 | Loss = 39.05\n",
      "Epoch = 24 | Loss = 38.05\n",
      "Epoch = 25 | Loss = 38.04\n",
      "Epoch = 26 | Loss = 37.00\n",
      "Epoch = 27 | Loss = 37.34\n",
      "Epoch = 28 | Loss = 36.58\n",
      "Epoch = 29 | Loss = 35.86\n",
      "Epoch = 30 | Loss = 35.09\n",
      "Epoch = 31 | Loss = 34.53\n",
      "Epoch = 32 | Loss = 33.93\n",
      "Epoch = 33 | Loss = 33.44\n",
      "Epoch = 34 | Loss = 32.83\n",
      "Epoch = 35 | Loss = 32.45\n",
      "Epoch = 36 | Loss = 31.29\n",
      "Epoch = 37 | Loss = 30.62\n",
      "Epoch = 38 | Loss = 30.04\n",
      "Epoch = 39 | Loss = 29.74\n",
      "Epoch = 40 | Loss = 29.14\n",
      "Epoch = 41 | Loss = 27.91\n",
      "Epoch = 42 | Loss = 27.71\n",
      "Epoch = 43 | Loss = 27.43\n",
      "Epoch = 44 | Loss = 26.29\n",
      "Epoch = 45 | Loss = 25.38\n",
      "Epoch = 46 | Loss = 25.30\n",
      "Epoch = 47 | Loss = 24.35\n",
      "Epoch = 48 | Loss = 23.21\n",
      "Epoch = 49 | Loss = 22.78\n",
      "Epoch = 50 | Loss = 22.16\n",
      "Epoch = 51 | Loss = 21.62\n",
      "Epoch = 52 | Loss = 21.17\n",
      "Epoch = 53 | Loss = 20.86\n",
      "Epoch = 54 | Loss = 20.33\n",
      "Epoch = 55 | Loss = 19.86\n",
      "Epoch = 56 | Loss = 19.21\n",
      "Epoch = 57 | Loss = 19.09\n",
      "Epoch = 58 | Loss = 18.47\n",
      "Epoch = 59 | Loss = 18.09\n",
      "Epoch = 60 | Loss = 17.29\n",
      "Epoch = 61 | Loss = 16.92\n",
      "Epoch = 62 | Loss = 16.74\n",
      "Epoch = 63 | Loss = 16.19\n",
      "Epoch = 64 | Loss = 16.04\n",
      "Epoch = 65 | Loss = 15.51\n",
      "Epoch = 66 | Loss = 15.02\n",
      "Epoch = 67 | Loss = 14.62\n",
      "Epoch = 68 | Loss = 14.15\n",
      "Epoch = 69 | Loss = 14.20\n",
      "Epoch = 70 | Loss = 13.70\n",
      "Epoch = 71 | Loss = 13.48\n",
      "Epoch = 72 | Loss = 13.31\n",
      "Epoch = 73 | Loss = 12.94\n",
      "Epoch = 74 | Loss = 12.55\n",
      "Epoch = 75 | Loss = 12.28\n",
      "Epoch = 76 | Loss = 12.06\n",
      "Epoch = 77 | Loss = 11.99\n",
      "Epoch = 78 | Loss = 11.53\n",
      "Epoch = 79 | Loss = 11.26\n",
      "Epoch = 80 | Loss = 11.00\n",
      "Epoch = 81 | Loss = 10.77\n",
      "Epoch = 82 | Loss = 10.68\n",
      "Epoch = 83 | Loss = 10.19\n",
      "Epoch = 84 | Loss = 10.11\n",
      "Epoch = 85 | Loss = 9.90\n",
      "Epoch = 86 | Loss = 9.76\n",
      "Epoch = 87 | Loss = 9.42\n",
      "Epoch = 88 | Loss = 9.31\n",
      "Epoch = 89 | Loss = 9.12\n",
      "Epoch = 90 | Loss = 9.06\n",
      "Epoch = 91 | Loss = 8.65\n",
      "Epoch = 92 | Loss = 8.51\n",
      "Epoch = 93 | Loss = 8.59\n",
      "Epoch = 94 | Loss = 8.39\n",
      "Epoch = 95 | Loss = 8.19\n",
      "Epoch = 96 | Loss = 7.92\n",
      "Epoch = 97 | Loss = 7.60\n",
      "Epoch = 98 | Loss = 7.69\n",
      "Epoch = 99 | Loss = 7.36\n"
     ]
    }
   ],
   "source": [
    "def train(ex, model, optim):\n",
    "    model.train()\n",
    "    \n",
    "    x, x_lens, x_mask, y, y_mask, y_in_x_inds = ex\n",
    "    \n",
    "    # Variable(x.cuda()) if using GPU\n",
    "    x, x_lens, x_mask, y, y_mask, y_in_x_inds = Variable(x), Variable(x_lens), Variable(x_mask), Variable(y), Variable(y_mask), Variable(y_in_x_inds)\n",
    "    \n",
    "    encoder_outputs = model.encode(x) # (batch_size, seq_len, hidden_dim*2)\n",
    "    encoder_proj_outputs = model.enc_to_dec(encoder_outputs) # (batch_size, seq_len, hidden_dim)\n",
    "    \n",
    "    ###CHANGE: make use of x_lens to index the last hidden states\n",
    "    batch_size = x.size(0)\n",
    "    seq_len = x.size(1)\n",
    "    h_0 = torch.index_select(encoder_proj_outputs.view(batch_size*seq_len,-1),0,x_lens) # be careful when input sequences have paddings\n",
    "    \n",
    "    c_0 = Variable(torch.zeros(h_0.size(0), h_0.size(1)).zero_()) \n",
    "    hidden = (h_0, c_0)\n",
    "    \n",
    "    p_y_seq = []\n",
    "    for i in range(y.size(1)):\n",
    "        #output = model.decode(hidden) \n",
    "        #y_emb = model.out_embedding(y[:, i]) # y_emb: (batch_size, embedding_dim)        \n",
    "        #hidden = model.decoder(y_emb, hidden) # (h_t, c_t): (batch_size, hidden_dim)\n",
    "        \n",
    "        ###CHANGE: update the decode function, move the code that uses y[:, i] down\n",
    "        output, context_t = model.decode(encoder_outputs, encoder_proj_outputs, x_mask, hidden) # with attention\n",
    "        \n",
    "        ###compute the next hidden state using the current output y[:, i]\n",
    "        y_emb = model.out_embedding(y[:, i]) # y_emb: (batch_size, embedding_dim)\n",
    "        hidden = model.decoder(torch.cat([y_emb, context_t], 1), hidden) \n",
    "        \n",
    "        p_y_t = output.gather(1, y[:, i].unsqueeze(1)) # (batch_size, 1)\n",
    "        \n",
    "        if model.copying:\n",
    "            copy_dist = output[:, model.out_vocab_size:model.out_vocab_size + y_in_x_inds.size(2)] # (batch_size, input_len)\n",
    "            # (batch_size, 1, input_len), (batch_size, input_len, 1)\n",
    "            copying_p_y_t = torch.bmm(copy_dist.unsqueeze(1), y_in_x_inds[:, i].unsqueeze(2)).squeeze(2)\n",
    "            p_y_t = p_y_t + copying_p_y_t\n",
    "                \n",
    "        p_y_seq.append(p_y_t)\n",
    "\n",
    "    p_y_seq = torch.cat([_ for _ in p_y_seq], 1) # (batch_size, seq_len)\n",
    "    p_y_seq = torch.masked_select(p_y_seq, y_mask)\n",
    "    loss = -torch.sum(torch.log(p_y_seq))/y.size(0) # loss = -\\sum_i log p(y|x)\n",
    "\n",
    "    # Clear gradients and run backward\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients, max_norm * v/||v|| if ||v|| > max_norm\n",
    "    torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=10.0)\n",
    "\n",
    "    # Update parameters\n",
    "    optim.step()\n",
    "    \n",
    "    return loss.data[0]\n",
    "\n",
    "#model = Seq2Seq(50, 20, input_vocab, output_vocab)\n",
    "model = AttentionSeq2Seq(50, 20, input_vocab, output_vocab, True)\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "# training loop\n",
    "n_epochs = 100\n",
    "for e in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "    for ex in train_loader:\n",
    "        l = train(ex, model, optim)\n",
    "        train_loss += l\n",
    "    print(\"Epoch = %d | Loss = %.2f\" % (e, train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2.3: Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model, similar to training. Using greedy search to infer the most likely sequence output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold:  max( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  min( WeatherHistory [ 'Temperature' ] ) <end> ) <end> ) <end> ) <end> ) <end>\n",
      "Gold:  max( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  min( WeatherHistory [ 'Temperature' ] ) <end> ) <end> ) <end> ) <end> ) <end>\n",
      "Gold:  min( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  min( WeatherHistory [ 'Temperature' ] ) <end> ) <end> ) <end> ) <end> ) <end>\n",
      "Gold:  lm( WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'Humidity' ] ) <end>\n",
      "Predict:  cor( WeatherHistroy [ 'Temperature' ] ) <end> ) <end> <end> <end> ) <end> <end> <end>\n",
      "Gold:  max( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  min( WeatherHistory [ 'Temperature' ] ) <end> ) <end> ) <end> ) <end> ) <end>\n",
      "Gold:  mean( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  predict( mod , WeatherHistory [ 'Temperature' ] ) <end> ) <end> ) <end> ) <end>\n",
      "Gold:  lm( WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'Humidity' ] ) <end>\n",
      "Predict:  cor( WeatherHistroy [ 'Temperature' ] , WeatherHistory [ 'FellingTemperature' ] ) <end> ] ) <end>\n",
      "Gold:  lm( WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'Humidity' ] ) <end>\n",
      "Predict:  lm( WeatherHistory [ 'Temperature' ] ) <end> ] ) <end> <end> ) <end> <end> )\n",
      "Gold:  cor( WeatherHistroy [ 'Temperature' ] , WeatherHistory [ 'FellingTemperature' ] ) <end>\n",
      "Predict:  cor( WeatherHistroy [ 'Temperature' ] , WeatherHistory [ 'FellingTemperature' ] ) <end> ) <end> <end>\n",
      "Gold:  max( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  min( WeatherHistory [ 'Temperature' ] ) <end> ) <end> ) <end> ) <end> ) <end>\n",
      "Gold:  mean( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  mean( WeatherHistory [ 'Temperature' ] ) <end> <end> <end> ) <end> <end> <end> <end> <end>\n",
      "Gold:  max( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  min( WeatherHistory [ 'Temperature' ] ) <end> ) <end> ) <end> ) <end> ) <end>\n",
      "Gold:  predict( mod , 12 ) <end>\n",
      "Predict:  predict( mod , 12 ) <end> ) <end> ) <end> ) <end> ) <end> )\n",
      "Gold:  lm( WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'Humidity' ] ) <end>\n",
      "Predict:  cor( WeatherHistroy [ 'Temperature' ] ) <end> ) <end> ) <end> ) <end> ) <end>\n",
      "Gold:  lm( WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'Humidity' ] ) <end>\n",
      "Predict:  cor( WeatherHistroy [ 'Temperature' ] , WeatherHistory [ 'FellingTemperature' ] ) <end> ] ) <end>\n",
      "Gold:  lm( WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'Humidity' ] ) <end>\n",
      "Predict:  lm( WeatherHistory [ 'Temperature' ] ) <end> ) <end> ] ) <end> ] ) <end>\n",
      "Gold:  max( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  min( WeatherHistory [ 'Temperature' ] ) <end> ) <end> ) <end> ) <end> ) <end>\n",
      "Gold:  cor( WeatherHistroy [ 'Temperature' ] , WeatherHistory [ 'FellingTemperature' ] ) <end>\n",
      "Predict:  cor( WeatherHistroy [ 'Temperature' ] , WeatherHistory [ 'FellingTemperature' ] ) <end> ) <end> )\n",
      "Gold:  cor( WeatherHistroy [ 'Temperature' ] , WeatherHistory [ 'FellingTemperature' ] ) <end>\n",
      "Predict:  cor( WeatherHistroy [ 'Temperature' ] , WeatherHistory [ 'FellingTemperature' ] ) <end> ] ) <end>\n",
      "Gold:  lm( WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'Humidity' ] ) <end>\n",
      "Predict:  cor( WeatherHistroy [ 'Temperature' ] ) <end> ) <end> <end> ) <end> <end> ) <end>\n",
      "Gold:  max( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  max( WeatherHistory [ 'Temperature' ] ) <end> ) <end> ) <end> ) <end> ) <end>\n",
      "Gold:  max( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  mean( WeatherHistory [ 'Temperature' ] ) <end> ) <end> ) <end> ) <end> ) <end>\n",
      "Gold:  predict( mod , 12 ) <end>\n",
      "Predict:  lm( WeatherHistory [ 'Temperature' ] ) <end> ) <end> ) <end> ) <end> ) <end>\n",
      "Gold:  cor( WeatherHistroy [ 'Temperature' ] , WeatherHistory [ 'FellingTemperature' ] ) <end>\n",
      "Predict:  cor( WeatherHistroy [ 'Temperature' ] , WeatherHistory [ 'FellingTemperature' ] ) <end> <end> <end> ]\n",
      "Gold:  min( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  min( WeatherHistory [ 'Temperature' ] ) <end> ) <end> ) <end> ) <end> ) <end>\n",
      "Gold:  min( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  min( WeatherHistory [ 'Temperature' ] ) <end> ) <end> <end> ) <end> <end> ) <end>\n",
      "Gold:  max( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  min( WeatherHistory [ 'Temperature' ] ) <end> ) <end> ) <end> ) <end> ) <end>\n",
      "Gold:  max( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  mean( WeatherHistory [ 'Temperature' ] ) <end> ) <end> <end> <end> <end> <end> ) <end>\n",
      "Gold:  cor( WeatherHistroy [ 'Temperature' ] , WeatherHistory [ 'FellingTemperature' ] ) <end>\n",
      "Predict:  cor( WeatherHistroy [ 'Temperature' ] , WeatherHistory [ 'FellingTemperature' ] ) <end> ) <end> <end>\n",
      "Gold:  cor( WeatherHistroy [ 'Temperature' ] , WeatherHistory [ 'FellingTemperature' ] ) <end>\n",
      "Predict:  min( WeatherHistory [ 'Temperature' ] ) <end> ) <end> ) <end> ) <end> ) <end>\n",
      "Gold:  mean( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  mean( WeatherHistory [ 'Temperature' ] ) <end> ) <end> ) <end> ) <end> ) <end>\n",
      "Gold:  max( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  min( WeatherHistory [ 'Temperature' ] ) <end> ) <end> ) <end> ) <end> ) <end>\n",
      "Gold:  lm( WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'Humidity' ] ) <end>\n",
      "Predict:  lm( WeatherHistory [ 'Temperature' ] ) <end> ) <end> ) <end> ) <end> ) <end>\n",
      "Gold:  lm( WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'Humidity' ] ) <end>\n",
      "Predict:  cor( WeatherHistroy [ 'Temperature' ] , WeatherHistory [ 'FellingTemperature' ] ) <end> ] ) <end>\n",
      "Gold:  mean( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  mean( WeatherHistory [ 'Temperature' ] ) <end> ) <end> ) <end> ) <end> ) <end>\n",
      "Gold:  mean( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  mean( WeatherHistory [ 'Temperature' ] ) <end> ) <end> ) <end> ) <end> ) <end>\n",
      "Gold:  max( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  mean( WeatherHistory [ 'Temperature' ] ) <end> ) <end> <end> <end> ) <end> <end> <end>\n",
      "Gold:  min( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  min( WeatherHistory [ 'Temperature' ] ) <end> ) <end> <end> ) <end> <end> ) <end>\n",
      "Gold:  min( WeatherHistory [ 'Humidity' ] ) <end>\n",
      "Predict:  ) <end> WeatherHistory [ 'Temperature' ] ) <end> ) <end> ) <end> ) <end> )\n",
      "Test accuracy: 0.38461538461538464\n"
     ]
    }
   ],
   "source": [
    "def test_batch(data_loader, model, max_len=15):\n",
    "    model.eval()\n",
    "    \n",
    "    num_correct = 0\n",
    "    for ex in data_loader:\n",
    "        x, x_lens, x_mask, y, y_mask, y_in_x_inds = ex \n",
    "        \n",
    "        x, x_lens, x_mask = Variable(x), Variable(x_lens), Variable(x_mask)\n",
    "    \n",
    "        encoder_outputs = model.encode(x) # (batch_size, seq_len, hidden_dim*2)\n",
    "        encoder_proj_outputs = model.enc_to_dec(encoder_outputs) # (batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        ###CHANGE: make use of x_lens to index the last hidden states\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "        h_0 = torch.index_select(encoder_proj_outputs.view(batch_size*seq_len,-1),0,x_lens) # be careful when input sequences have paddings\n",
    "    \n",
    "        c_0 = Variable(torch.zeros(h_0.size(0), h_0.size(1)).zero_()) \n",
    "        hidden = (h_0, c_0)\n",
    "        \n",
    "        ###CHANGE: start with empty prediction\n",
    "        seq = []\n",
    "        for i in range(max_len):\n",
    "            #output = model.decode(hidden) \n",
    "            \n",
    "            ###CHANGE: update the decode function, move the code that uses y[:, i] down\n",
    "            output, context_t = model.decode(encoder_outputs, encoder_proj_outputs, x_mask, hidden) # with attention\n",
    "        \n",
    "            sampleLogprobs, it = torch.max(output.data, 1)\n",
    "            y_t = it.view(-1).long()\n",
    "            seq.append(y_t)\n",
    "            \n",
    "            if model.copying:\n",
    "                new_y_t = []\n",
    "                for j in range(y_t.size(0)):\n",
    "                    if y_t[j] < model.out_vocab_size:\n",
    "                        new_y_t.append(y_t[j])\n",
    "                    else:\n",
    "                        k = x.data[j, y_t[j]-model.out_vocab_size]\n",
    "                        w = model.input_vocab.get_word(k)\n",
    "                        new_k = model.output_vocab.get_index(w)\n",
    "                        new_y_t.append(new_k)\n",
    "                y_t = torch.LongTensor(new_y_t)\n",
    "            \n",
    "            ###compute the next hidden state using the current output y_t\n",
    "            y_prev = Variable(y_t)\n",
    "            y_emb = model.out_embedding(y_prev) # y_emb: (batch_size, embedding_dim)\n",
    "            hidden = model.decoder(torch.cat([y_emb, context_t], 1), hidden) \n",
    "            \n",
    "            #hidden = model.decoder(y_emb, hidden)\n",
    "        \n",
    "        pred_y = torch.cat([_.unsqueeze(1) for _ in seq], 1)\n",
    "        \n",
    "        for idx in range(batch_size):\n",
    "            gold_toks = []\n",
    "            for wi in y[idx].tolist():\n",
    "                gold_toks.append(model.output_vocab.get_word(wi))\n",
    "            print(\"Gold: \", ' '.join(gold_toks))\n",
    "        \n",
    "            pred_toks = []\n",
    "            for wi in pred_y[idx].tolist():\n",
    "                #w = model.output_vocab.get_word(wi)\n",
    "            \n",
    "                if wi < model.out_vocab_size:\n",
    "                    w = model.output_vocab.get_word(wi)\n",
    "                else:\n",
    "                    w = model.input_vocab.get_word(x.data[idx][wi-model.out_vocab_size])\n",
    "                    #print(\"copying \", w)\n",
    "                    \n",
    "                pred_toks.append(w)\n",
    "                \n",
    "            print(\"Predict: \",' '.join(pred_toks))\n",
    "            \n",
    "            for i in range(len(gold_toks)):\n",
    "                g_tok = gold_toks[i]\n",
    "                p_tok = pred_toks[i]\n",
    "                if (g_tok != p_tok):\n",
    "                    break\n",
    "                elif (g_tok == \"<end>\"):\n",
    "                    num_correct += 1\n",
    "                    \n",
    "                    \n",
    "    print(\"Test accuracy: {}\".format(num_correct / len(data_loader)))\n",
    "                \n",
    "        \n",
    "test_batch(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
