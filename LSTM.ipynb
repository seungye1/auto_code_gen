{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f1315812b50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy_mappings = dict()\n",
    "# mean = set([\"mean\", \"average\", \"center\", \"typical\"])\n",
    "# minimum = set([\"minimum\", \"lowest\", \"smallest\"])\n",
    "# maximum = set([\"maximum\", \"biggest\", \"highest\", \"largest\"])\n",
    "# ranges = set([\"range\", \"ranges\", \"span\", \"spans\"])\n",
    "# corr = set([\"correlation\", 'corr', 'cor'])\n",
    "# std = set(['sd', 'std'])\n",
    "# var = set(['variance', 'var'])\n",
    "# quantile = set(['quantile'])\n",
    "# geq = set(['greater than or equal to', \"higher than or equal to\", \"bigger than or equal to\", \">=\"])\n",
    "# leq = set(['less than or equal to', \"smaller than or equal to\", \"<=\"])\n",
    "# less = set(['less than', 'smaller than', '<'])\n",
    "# greater = set(['greater than', 'bigger than', 'higher than', \">\"])\n",
    "# equal1 = set([ \"==\"])\n",
    "# equal2 = set([\"equal\",\"=\"])\n",
    "\n",
    "# describe = set(['summary', 'summarize', 'describe'])\n",
    "# linreg_fit = set([\"linear regression\", \"relationship\", \"fit\"])\n",
    "# predict = set([\"predict\", \"forecast\", \"predicted\", \"predicting\"])\n",
    "# pltplot_hist = set(['histogram', 'hist', 'distribution'])\n",
    "# pltplot_boxplot = set(['boxplot'])\n",
    "# pltplot_scatter = set(['scatter', 'scatterplot'])\n",
    "\n",
    "# mean = {key:\"mean(\" for key in mean}\n",
    "# minimum = {key:\"minimum(\" for key in minimum}\n",
    "# maximum = {key:\"maximum(\" for key in maximum}\n",
    "# ranges = {key:\"ranges(\" for key in ranges}\n",
    "# corr = {key:\"corr(\" for key in corr}\n",
    "# std = {key:\"std(\" for key in std}\n",
    "# var = {key:\"var(\" for key in var}\n",
    "# quantile = {key:\"quantile(\" for key in quantile}\n",
    "# greater = {key:\"greater(\" for key in greater}\n",
    "# less = {key:\"less(\" for key in less}\n",
    "# equal2 = {key:\"equal(\" for key in equal2}\n",
    "# describe = {key:\"describe(\" for key in describe}\n",
    "# linreg_fit = {key:\"lr(\" for key in linreg_fit}\n",
    "# predict = {key:\"predict(\" for key in predict}\n",
    "# histogram = {key:\"histogram(\" for key in pltplot_hist}\n",
    "# scatter = {key:\"scatter(\" for key in pltplot_scatter}\n",
    "# boxplot = {key:\"boxplot(\" for key in pltplot_boxplot}\n",
    "\n",
    "# geq = {key:\"geq(\" for key in geq}\n",
    "# leq = {key:\"leq(\" for key in leq}\n",
    "# equal1 = {key:\"equal(\" for key in equal1}\n",
    "\n",
    "# round1 = {**geq, **leq, **equal1}\n",
    "# round2 = {**mean, **minimum, **maximum, **ranges, **corr, **std, **var, **quantile, **greater, **less, **equal2,\n",
    "#          **describe, **linreg_fit, **predict, **histogram, **scatter, **boxplot}\n",
    "\n",
    "# print(round1, round2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4326      number of data with total_sulfur_dioxide lower...\n",
       "15725     give the average compactness in the breast_can...\n",
       "83905     describe the relationship of density and ph ba...\n",
       "112329    does loan have an affect on day_of_week in the...\n",
       "80267     whatâ€™s the highest concavity today in the brea...\n",
       "56536     give me the actual maint and buying in the car...\n",
       "37440     what's the predicted value of symmetry when co...\n",
       "26148     what's the coldest default in the bank_marketi...\n",
       "86708     give me the correlation between dmc and x in t...\n",
       "20306     what is your prediction of total_sulfur_dioxid...\n",
       "70022     what's the hottest day_of_week in the bank_mar...\n",
       "68311           give the mean age of the data  < data >  . \n",
       "13577     what is the linear model of maint predicting b...\n",
       "35595     find the persons linear model at -18.279824249...\n",
       "100265    can you tell me difference in relationship of ...\n",
       "102397    which listed age is the greatest in the heart_...\n",
       "6883                highest of contact in df bank_marketing\n",
       "99971     how are persons and safety correlated in the c...\n",
       "34743           what's the mean hue in the wine dataset ?  \n",
       "7520             hist of cons.conf.idx in df bank_marketing\n",
       "73100     what is the highest workclass recorded in the ...\n",
       "5639      how many info where rh higher than or equal to...\n",
       "70023     give me the highest day_of_week degree in the ...\n",
       "105023    of the listed  < var1 >  ,  what is the mean i...\n",
       "453                       smallest of the age in data adult\n",
       "103345    what is the lowest fbs value in the heart_dise...\n",
       "94542     when workclass is 4 degrees  ,  what will the ...\n",
       "13734     based on the regression can you find the predi...\n",
       "15156     what is the symmetry when the perimeter is -25...\n",
       "92470     is the poutcome accurate to how it feels in th...\n",
       "                                ...                        \n",
       "27682     when the pdays is  < val >  ,  what is the val...\n",
       "47049     find the total mean job in the bank_marketing ...\n",
       "8305      what is the minimum petal_width entry in the i...\n",
       "99806     when doors is -8.337919222898165 degrees  ,  w...\n",
       "65922     tell me the average temp in the forest_fire da...\n",
       "101752    given that concave points is 86.65151453142616...\n",
       "54899     predict the flavanoids when alcalinity_of_ash ...\n",
       "89266     how hot does shucked_weight get in the abalone...\n",
       "95351     what is the maximum of occupation in the adult...\n",
       "48136     what is the correlation between contact and pr...\n",
       "106556    what is the 561_feature at the predicted triax...\n",
       "72430     what is the lowest petal_length of the iris da...\n",
       "49604     does the feeling_weather like what was forecas...\n",
       "93980     can i have the forecasted sepal_length level f...\n",
       "110658    can a change in shucked_weight predict a chang...\n",
       "81448     given that trestbps is 46.292203568165604 pred...\n",
       "22572       what's mean of the dc in the < data >  data set\n",
       "56595         find the hottest comfort in the car database \n",
       "1480      number of data where alcohol less than 44.0417...\n",
       "105578    find the correlation between ph and free_sulfu...\n",
       "88301     of the listed sex values ,  which is the lowes...\n",
       "63117     tell me what chlorides is outside ,  and what'...\n",
       "107098    give me the highest identifier value in the sm...\n",
       "1910      amount of the nonflavanoid_phenols in wine dat...\n",
       "87783     what is the average wind in the forest_fire da...\n",
       "4787      how many data  triaxial angular velocity great...\n",
       "89182     give the minimum shucked_weight of the data  <...\n",
       "75850     what's the hottest malic_acid in the wine data...\n",
       "72438     tell me the numeric correlation value of petal...\n",
       "53615     what is the maximum of capital_loss in the adu...\n",
       "Length: 91898, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "df = pd.read_csv(\"data/codegen.csv\")\n",
    "# df.head()\n",
    "data_x = df[\"utterance\"]\n",
    "data_y = df[\"targets\"]\n",
    "\n",
    "\n",
    "new_data_x = []\n",
    "# new_data_y = []\n",
    "for utterance in data_x:\n",
    "    sentence = utterance.lower()\n",
    "    sentence = sentence.replace(\">=\", \" >= \") if sentence.find(\">=\") != -1 else sentence.replace(\">\", \" > \")\n",
    "    sentence = sentence.replace(\"<=\", \" <= \") if sentence.find(\"<=\") != -1 else sentence.replace(\"<\", \" < \")\n",
    "    sentence = sentence.replace(\"==\", \" == \") if sentence.find(\"==\") != -1 else sentence.replace(\"=\", \"=\")\n",
    "    if sentence.find(\">=\") == -1 and sentence.find(\"<=\") == -1 and sentence.find('==') == -1 and sentence.find('=') != -1:\n",
    "        sentence = sentence.replace(\"=\", \" = \")\n",
    "    if sentence.strip()[-1] == \".\":\n",
    "        sentence = sentence[:-1] + \" . \"\n",
    "    punct_table = str.maketrans({key: \" \" + key + \" \" for key in string.punctuation if key != '_' and key != \"'\" and key != \">\" and key != \"=\" and key != \"<\" and key != \"-\" and key != '.'})\n",
    "    sentence = sentence.translate(punct_table)\n",
    "    new_data_x.append(sentence)\n",
    "    \n",
    "\n",
    "#     sentence = sentence.replace(\"higher than or equal to\", \" geq( \")\n",
    "#     sentence = sentence.replace(\"standard deviation\", \" geq( \")\n",
    "#     sentence = \"%s\" % (utterance) \n",
    "#     for paraphrasing in round1.keys():\n",
    "#         if paraphrasing in utterance:\n",
    "#             sentence = sentence.replace(paraphrasing, \" \"+ round1[paraphrasing] + \" \")\n",
    "#     for paraphrasing in round2.keys():\n",
    "#         if paraphrasing in utterance:\n",
    "#             sentence = sentence.replace(paraphrasing, \" \" + round2[paraphrasing] + \" \")\n",
    "# #     word_list = []\n",
    "# #     for word in utterance.split(' '):\n",
    "# #         if word in round1.keys():\n",
    "# #             word_list.append(round1[word])\n",
    "# #         elif word in round2.keys():\n",
    "# #             word_list.append(round2[word])\n",
    "# #         else:\n",
    "# #             word_list.append(word)\n",
    "        \n",
    "# #     spaced_ex = ' '.join(word_list)\n",
    "#     print(sentence)\n",
    "#     punct_table = str.maketrans({key: \" \" + key + \" \" for key in string.punctuation if key != '_' and key != \"'\"})\n",
    "#     sentence = sentence.translate(punct_table)\n",
    "    \n",
    "#     new_data_x.append(sentence)\n",
    "    \n",
    "# split into test/train data\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y = train_test_split(pd.Series(new_data_x), data_y, test_size=0.2)\n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy_mappings = dict()\n",
    "# mean = set([\"average\", \"median\", \"center\", \"typical\"])\n",
    "# minimum = set([\"minimum\", \"lowest\", \"smallest\"])\n",
    "# maximum = set([\"maximum\", \"biggest\", \"highest\", \"largest\"])\n",
    "# ranges = set([\"range\", \"ranges\", \"span\", \"spans\"])\n",
    "# corr = set([\"correlation\", 'corr', 'cor'])\n",
    "# std = set([\"standard deviation\"])\n",
    "# var = set(['variance', 'var'])\n",
    "# quantile = set(['quantile'])\n",
    "# geq = set(['greater than or equal to', \"higher than or equal to\", \"bigger than or equal to\"])\n",
    "# leq = set(['less than or equal to', \"smaller than or equal to\"])\n",
    "# less = set(['less than', 'smaller than'])\n",
    "# greater = set(['greater than', 'bigger than', 'higher than'])\n",
    "# describe = set(['summary', 'summarize', 'describe'])\n",
    "# linreg_fit = set([\"linear regression\", \"relationship\", \"fit\"])\n",
    "# predict = set([\"predict\", \"forecast\", \"predicted\", \"predicting\"])\n",
    "# pltplot_hist = set(['histogram', 'hist', 'distribution'])\n",
    "# pltplot_boxplot = set(['boxplot'])\n",
    "# pltplot_scatter = set(['scatter', 'scatterplot'])\n",
    "\n",
    "\n",
    "# copy_mappings = {\"mean(\":mean, \"minimum(\":minimum, \"maximum(\":maximum, 'ranges(':ranges, \n",
    "#                  \"corr(\":corr, \"predict(\":predict, \"std(\":std, \"variance(\":var, \"quantile(\":quantile, \n",
    "#                  \"describe(\":describe, \"lr(\":linreg_fit, \"histogram(\":pltplot_hist,\n",
    "#                 \"boxplot(\":pltplot_boxplot, \"scatterplot(\":pltplot_scatter}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building input and output vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NULL': 0, 'UNK': 1, '<end>': 2, 'number': 3, 'of': 4, 'data': 5, 'with': 6, 'total_sulfur_dioxide': 7, 'lower': 8, 'than': 9, 'dataset': 10, 'give': 11, 'the': 12, 'average': 13, 'compactness': 14, 'in': 15, 'breast_cancer': 16, '.': 17, 'describe': 18, 'relationship': 19, 'density': 20, 'and': 21, 'ph': 22, 'based': 23, 'on': 24, 'their': 25, 'correlation': 26, 'wine_quality': 27, 'does': 28, 'loan': 29, 'have': 30, 'an': 31, 'affect': 32, 'day_of_week': 33, 'bank_marketing': 34, '?': 35, 'whatâ€™s': 36, 'highest': 37, 'concavity': 38, 'today': 39, 'me': 40, 'actual': 41, 'maint': 42, 'buying': 43, 'car': 44, \"what's\": 45, 'predicted': 46, 'value': 47, 'symmetry': 48, 'when': 49, 'concave': 50, 'points': 51, 'is': 52, 'degrees': 53, 'coldest': 54, 'default': 55, 'between': 56, 'dmc': 57, 'x': 58, 'forest_fire': 59, 'dataset.': 60, 'what': 61, 'your': 62, 'prediction': 63, 'alcohol': 64, 'hottest': 65, 'mean': 66, 'age': 67, '<': 68, '>': 69, 'linear': 70, 'model': 71, 'predicting': 72, 'find': 73, 'persons': 74, 'at': 75, 'can': 76, 'you': 77, 'tell': 78, 'difference': 79, 'lug_boot': 80, 'safety': 81, 'which': 82, 'listed': 83, 'greatest': 84, 'heart_disease': 85, 'contact': 86, 'df': 87, 'how': 88, 'are': 89, 'correlated': 90, 'hue': 91, 'wine': 92, 'hist': 93, 'cons.conf.idx': 94, 'workclass': 95, 'recorded': 96, 'adult': 97, 'many': 98, 'info': 99, 'where': 100, 'rh': 101, 'higher': 102, 'or': 103, 'equal': 104, 'to': 105, 'degree': 106, 'var1': 107, ',': 108, 'smallest': 109, 'lowest': 110, 'fbs': 111, '4': 112, 'will': 113, 'capital_loss': 114, 'be': 115, 'regression': 116, 'comfort': 117, 'perimeter': 118, 'poutcome': 119, 'accurate': 120, 'it': 121, 'feels': 122, 'occupation': 123, 'create': 124, 'a': 125, 'volatile_acidity': 126, 'through': 127, 'residual_sugar': 128, 'frame': 129, 'there': 130, 'malic_acid': 131, 'fractal': 132, 'dimension': 133, 'area': 134, '=': 135, 'max': 136, 'all': 137, 'sepal_length': 138, 'iris': 139, 'identifier': 140, '8': 141, '561_feature': 142, 'smartphones': 143, 'emp.var.rate': 144, 'calculate': 145, 'variables': 146, 'height': 147, 'abalone': 148, 'make': 149, 'month': 150, 'free_sulfur_dioxide': 151, 'please': 152, 'values': 153, 'for': 154, 'described': 155, 'previous': 156, 'question': 157, 'database': 158, 'proanthocyanins': 159, 'nonflavanoid_phenols': 160, 'maximum': 161, 'petal_length': 162, 'from': 163, 'education': 164, 'sex': 165, 'campaign': 166, 'euribor3m': 167, 'viscera': 168, 'total': 169, 'temp': 170, 'high': 171, 'pdays': 172, 'go': 173, 'minimum': 174, 'trestbps': 175, 'triaxial': 176, 'acceleration': 177, 'entry': 178, 'fit': 179, 'ash': 180, 'job': 181, 'length': 182, 'sepal_width': 183, 'cold': 184, 'get': 185, 'marital': 186, 'influence': 187, 'doors': 188, 'do': 189, 'see': 190, 'any': 191, 'magnesium': 192, 'flavanoids': 193, 'label': 194, 'ffmc': 195, 'was': 196, 'day': 197, 'feeling_weather': 198, 'like': 199, 'forecasted': 200, 'as': 201, 'wind': 202, 'native_country': 203, 'by': 204, 'creating': 205, 'i': 206, 'isi': 207, 'level': 208, \"it's\": 209, 'dc': 210, 'duration': 211, 'num': 212, 'cp': 213, 'usually': 214, 'look': 215, 'texture': 216, '6': 217, 'extent': 218, 'radius': 219, 'rain': 220, 'chol': 221, 'standard': 222, 'deviation': 223, 'capital_gain': 224, 'petal_width': 225, '1': 226, 'angular': 227, 'velocity': 228, 'citric_acid': 229, 'respond': 230, 'change': 231, 'quality': 232, 'marital_status': 233, 'fnlwgt': 234, 'education_num': 235, 'varor_intensity': 236, 'val': 237, 'did': 238, 'chlorides': 239, 'total_phenols': 240, 'match': 241, 'thalach': 242, 'using': 243, 'y': 244, 'race': 245, 'scatter': 246, 'plot': 247, 'b': 248, '/': 249, 'w': 250, '5': 251, 'shucked_weight': 252, 'sparsity': 253, 'cons.price.idx': 254, 'could': 255, 'would': 256, 'given': 257, 'restecg': 258, 'that': 259, 'predict': 260, 'whole_weight': 261, 'shell_weight': 262, 'entries': 263, '<=': 264, 'greater': 265, 'least': 266, 'house_serv': 267, 'hours_per_week': 268, 'strong': 269, 'has': 270, 'ever': 271, 'been': 272, 'hot': 273, 'gotten': 274, 'pm': 275, '(': 276, 'vice-versa': 277, ')': 278, 'typical': 279, 'diameter': 280, 'set': 281, 'alcalinity_of_ash': 282, 'corr': 283, 'quantity': 284, 'cor': 285, 'nr.employed': 286, 'take': 287, 'base': 288, 'span': 289, 'associated': 290, 'current': 291, 'store': 292, 'variable': 293, '{': 294, '}': 295, '_df': 296, 'beteeen': 297, 't': 298, 'smoothness': 299, 'outside': 300, '7': 301, 'equation': 302, 'boxplot': 303, 'notice': 304, '3': 305, 'check': 306, 'rows': 307, '>=': 308, 'bigger': 309, 'same': 310, 'feel': 311, '9': 312, '0': 313, 'spans': 314, 'fixed_acidity': 315, 'expected': 316, 'sulphates': 317, 'housing': 318, 'aggregate': 319, 'think': 320, 'should': 321, 'quantile': 322, 'numeric': 323, 'variance': 324, '2': 325, 'sum': 326, 'scatterplot': 327, 'sd': 328, 'temperature': 329, 'weatherhistory': 330, 'less': 331, 'much': 332, '==': 333, 'larger': 334, 'min': 335, 'histogram': 336, 'std': 337, 'smaller': 338, 'load': 339, 'ranges': 340, 'distribution': 341, 'range': 342, 'largest': 343, 'amount': 344, 'var': 345, '_data': 346, 'd': 347, 'humidity': 348, '12': 349, 'feeling': 350, 'celsius': 351, '_': 352, 'weather': 353, 'history': 354, 'summary': 355, 'Î²Ì‚': 356}\n",
      "{0: 'NULL', 1: 'UNK', 2: '<end>', 3: 'less', 4: '(', 5: 'data', 6: '=', 7: 'wine_quality', 8: ',', 9: 'var1', 10: 'total_sulfur_dioxide', 11: 'val', 12: ')', 13: 'mean', 14: '<data>,', 15: \"'compactness'\", 16: 'corr', 17: 'WeatherHistroy,', 18: \"'<var1>',\", 19: 'var2', 20: \"'pH'\", 21: \"'day_of_week'\", 22: 'max', 23: \"'concavity'\", 24: \"'buying'\", 25: 'lr_pred', 26: 'model', 27: 'linreg,', 28: 'newdata', 29: '{', 30: \"'concave\", 31: \"points'\", 32: ':', 33: '}', 34: \"'default'\", 35: \"'x'\", 36: \"'alcohol'\", 37: \"'age'\", 38: 'lr', 39: \"'persons'\", 40: \"'lug_boot'\", 41: 'maximum', 42: 'bank_marketing', 43: 'contact', 44: \"'safety'\", 45: \"'hue'\", 46: 'histogram', 47: 'cons.conf.idx', 48: \"'workclass'\", 49: 'greater_eq', 50: 'forest_fire', 51: 'RH', 52: \"'free_sulfur_dioxide'\", 53: 'minimum', 54: 'adult', 55: 'age', 56: 'min', 57: \"'fbs'\", 58: '4', 59: \"'comfort'\", 60: \"'perimeter'\", 61: \"'job'\", 62: \"'occupation'\", 63: \"'volatile_acidity'\", 64: \"'malic_acid'\", 65: \"'area'\", 66: \"'sepal_length'\", 67: \"'identifier'\", 68: '8', 69: \"'emp.var.rate'\", 70: \"'duration'\", 71: \"'sex'\", 72: \"'month'\", 73: \"'DMC'\", 74: '-92.28874131831061', 75: \"'nonflavanoid_phenols'\", 76: \"'maint'\", 77: \"'total_sulfur_dioxide'\", 78: \"'education'\", 79: '5', 80: \"'diameter'\", 81: \"'euribor3m'\", 82: \"'viscera'\", 83: \"'temp'\", 84: \"'pdays'\", 85: \"'height'\", 86: \"'trestbps'\", 87: \"'triaxial\", 88: \"acceleration'\", 89: \"'length'\", 90: \"'doors'\", 91: '45.97276422874273', 92: \"'symmetry'\", 93: \"'magnesium'\", 94: 'free_sulfur_dioxide', 95: \"'ash'\", 96: \"'label'\", 97: \"'sepal_width'\", 98: \"'FFMC'\", 99: \"'native_country'\", 100: \"'DC'\", 101: \"'cp'\", 102: 'workclass', 103: \"'day'\", 104: \"'texture'\", 105: '6', 106: \"'radius'\", 107: \"'poutcome'\", 108: \"'rain'\", 109: 'std', 110: 'abalone', 111: 'viscera', 112: \"'marital'\", 113: \"'capital_gain'\", 114: \"'petal_width'\", 115: '1', 116: \"'citric_acid'\", 117: 'euribor3m', 118: \"'marital_status'\", 119: \"'education_num'\", 120: \"'petal_length'\", 121: \"'varor_intensity'\", 122: 'equal', 123: 'heart_disease', 124: 'chol', 125: \"'chlorides'\", 126: 'angular', 127: \"velocity'\", 128: \"'relationship'\", 129: \"'fnlwgt'\", 130: \"'thalach'\", 131: '-42.82145357711058', 132: \"'campaign'\", 133: \"'y'\", 134: \"'cons.conf.idx'\", 135: \"'total_phenols'\", 136: \"'chol'\", 137: \"'561_feature'\", 138: 'scatterplot', 139: 'abalone,', 140: 'length', 141: \"'fractal\", 142: \"dimension'\", 143: \"'flavanoids'\", 144: \"'shucked_weight'\", 145: \"'cons.price.idx'\", 146: \"'restecg'\", 147: \"'residual_sugar'\", 148: \"'shell_weight'\", 149: \"'RH'\", 150: 'less_eq', 151: 'native_country', 152: \"'previous'\", 153: 'breast_cancer', 154: 'perimeter', 155: 'capital_gain', 156: \"'contact'\", 157: \"'house_serv'\", 158: \"'hours_per_week'\", 159: \"'capital_loss'\", 160: \"'whole_weight'\", 161: \"'wind'\", 162: 'FFMC', 163: '99.18971088265394', 164: \"'alcalinity_of_ash'\", 165: \"'proanthocyanins'\", 166: 'car,', 167: 'buying', 168: 'persons', 169: 'previous', 170: 'shucked_weight', 171: 'wine', 172: 'alcalinity_of_ash', 173: 'summation', 174: 'proanthocyanins', 175: 'y', 176: \"'nr.employed'\", 177: 'ranges', 178: 'symmetry', 179: '7', 180: 'load', 181: \"'ISI'\", 182: \"'smoothness'\", 183: 'wine_quality,', 184: 'chlorides', 185: 'density', 186: 'boxplot', 187: '3', 188: \"'race'\", 189: 'smartphones', 190: 'triaxial', 191: 'velocity', 192: 'wind', 193: 'adult,', 194: 'education_num', 195: '9', 196: '0', 197: '-62.28373340794866', 198: 'fixed_acidity', 199: 'citric_acid', 200: \"'num'\", 201: \"'sulphates'\", 202: 'bank_marketing,', 203: 'marital', 204: 'wine,', 205: 'ash', 206: 'varor_intensity', 207: 'forest_fire,', 208: 'rain', 209: 'DC', 210: \"'housing'\", 211: '-9.559450451788763', 212: 'shell_weight', 213: 'label', 214: '-31.083213239978846', 215: 'smartphones,', 216: '561_feature', 217: 'acceleration', 218: 'job', 219: 'nr.employed', 220: 'education', 221: \"'density'\", 222: 'emp.var.rate', 223: 'quantile', 224: 'house_serv', 225: \"'loan'\", 226: 'variance', 227: \"'quality'\", 228: '2', 229: 'capital_loss', 230: 'restecg', 231: 'breast_cancer,', 232: 'compactness', 233: 'radius', 234: 'residual_sugar', 235: '-34.295612629464145', 236: 'comfort', 237: 'lug_boot', 238: 'poutcome', 239: 'diameter', 240: \"'fixed_acidity'\", 241: '-17.146746929670996', 242: 'WeatherHistory,', 243: \"'Temperature'\", 244: 'alcohol', 245: 'whole_weight', 246: '77.38624162975162', 247: '44.64382628260253', 248: 'duration', 249: 'marital_status', 250: 'sex', 251: '-31.372360931256765', 252: '33.46704748155011', 253: 'occupation', 254: 'heart_disease,', 255: '-84.20583509899393', 256: '-88.95814035653169', 257: '43.303919478341044', 258: 'pH', 259: 'hours_per_week', 260: '11.349597207999778', 261: 'safety', 262: 'maint', 263: 'greater', 264: '76.51828386483231', 265: '35.630986085374815', 266: 'malic_acid', 267: 'ISI', 268: 'fnlwgt', 269: '77.07211564036018', 270: 'area', 271: 'concave', 272: 'points', 273: 'campaign', 274: 'month', 275: '-23.871447470053056', 276: 'car', 277: 'quality', 278: '67.11357425373257', 279: 'fractal', 280: 'dimension', 281: 'housing', 282: 'concavity', 283: 'cons.price.idx', 284: '83.8602376326424', 285: 'num', 286: '-96.87391114506629', 287: 'fbs', 288: 'doors', 289: '71.63136320251147', 290: '50.43617450179474', 291: 'iris', 292: 'sepal_length', 293: 'volatile_acidity', 294: 'trestbps', 295: '57.21558011396144', 296: '-99.34390097302885', 297: '18.7339353757135', 298: 'x', 299: '29.91887273357534', 300: '99.75201920581534', 301: '86.78862350524602', 302: 'identifier', 303: '-87.00732642984745', 304: \"'Temperature',\", 305: \"'FeelingTemperature'\", 306: '45.14560544749236', 307: '-32.06292841583203', 308: '86.59182065915203', 309: '38.387498316138846', 310: '93.16260660070162', 311: '-47.41869478395577', 312: '55.60903158705247', 313: '-77.38282173707007', 314: '3.4526321726152247', 315: '-41.93025313262502', 316: '-17.297604376040425', 317: '99.30429073857172', 318: '46.708514200832155', 319: 'iris,', 320: 'sepal_width', 321: 'petal_length', 322: '55.15323890940866', 323: 'flavanoids', 324: '-34.180698514868794', 325: 'smoothness', 326: '39.53096013369236', 327: '24.747642785951115', 328: '-79.84764449628355', 329: 'race', 330: 'nonflavanoid_phenols', 331: 'texture', 332: '-15.597299733903228', 333: 'pdays', 334: 'magnesium', 335: '36.835463747456515', 336: '24.49988192640066', 337: '60.13924726258472', 338: '-30.480622094243515', 339: 'petal_width', 340: 'thalach', 341: '76.19533409850004', 342: '40.848931051159724', 343: '23.097151491195007', 344: 'day', 345: '-8.790314298872758', 346: '2.6837798916269975', 347: '6.981051370605471', 348: 'total_phenols', 349: '43.16025197003276', 350: '-71.31842466683884', 351: 'DMC', 352: '34.8915069499231', 353: '-64.1230385765428', 354: '-65.91363292796623', 355: '96.34748362321349', 356: 'cp', 357: '-91.28212171563814', 358: '-86.45842235831984', 359: '-51.676601820259926', 360: '46.93232453742385', 361: '51.22867306130706', 362: '-3.4108762728380526', 363: '-80.07233437350816', 364: '7.594895389380767', 365: '-42.53168968976113', 366: '-66.5429530336314', 367: '-40.029574928541244', 368: '-86.58515975336334', 369: '63.67908757823801', 370: 'hue', 371: 'temp', 372: '-2.252644017775978', 373: '34.24066204082848', 374: '71.5788120733254', 375: '7.720423197595011', 376: '-22.227398632463718', 377: 'day_of_week', 378: '-55.88454981636482', 379: '-58.61262147274011', 380: '97.49006121785365', 381: '-10.659212615012663', 382: 'default', 383: \"'Humidity'\", 384: '-97.84859001597523', 385: '-86.31455476588428', 386: '-42.019850072970975', 387: '58.68985501544478', 388: '-44.068742947560224', 389: 'relationship', 390: '-50.62625822012643', 391: '-70.69834765872733', 392: '7.465055626208823', 393: '-74.65220370923025', 394: '20.396022893832153', 395: '-30.673584422634164', 396: 'loan', 397: '-33.91864430797766', 398: '45.85365707807327', 399: '85.22583332732594', 400: '3.3340694549170706', 401: '-32.06399549402738', 402: '-13.032202978043088', 403: '-38.407237038671546', 404: '2.7731219488554615', 405: '57.42017398256175', 406: '97.28531319030367', 407: '-1.1331705828855831', 408: '43.4257022608553', 409: '-94.84549355191018', 410: 'describe', 411: '39.87605061652968', 412: '16.640104181473703', 413: '41.82072161398921', 414: '73.61121690871792', 415: '-80.79459123882964', 416: '-15.482722801390352', 417: '-58.64364255570551', 418: '-81.09806845252794', 419: '-43.73224865536296', 420: '-88.09072624143343', 421: '51.84721296394878', 422: '87.05341843947932', 423: '-35.83012944181485', 424: '-28.298083988900117', 425: '-34.38389384708627', 426: '-40.56157169370398', 427: '-96.62904623755182', 428: '78.81371849230464', 429: '84.60670529799143', 430: 'sulphates', 431: '47.005909415905336', 432: '73.16948655021332', 433: '-1.956701334684368', 434: '59.54750832900194', 435: '37.307857227513296', 436: '-43.60012656201662', 437: 'height', 438: '61.16025242813126', 439: '-41.00160613875421', 440: '-16.036948863457255', 441: '-31.96940285397372', 442: '-80.3884887620125', 443: '-36.22522131126693', 444: '-35.4638038307214', 445: '-0.13836216747328933', 446: '-8.473172183975748', 447: '-46.09855322268439', 448: '57.89499062384854', 449: '95.95196333449954', 450: '-8.332982858021978', 451: '-5.419027680550556', 452: '59.95851485840282', 453: '92.82142675204398', 454: '-45.7685989295447', 455: '5.2805308639780435', 456: '7.192095991212028', 457: '-2.791314516392248', 458: '-35.78258059504489', 459: '-67.33505653043312', 460: '12', 461: '-21.851546886805195', 462: '-72.80892386318867', 463: '89.5389877660738', 464: '-53.7056118339903', 465: '-3.144305090278749', 466: '82.50216785406326', 467: '48.356830412910085', 468: '76.20399056048831', 469: '-54.96044707686225', 470: '12.160675755839193', 471: '34.55899106112838', 472: '90.36523588814364', 473: '-24.814660577835994', 474: '-67.65678702883535', 475: '3.6944528225484277', 476: '75.2800887484672', 477: '-86.7598483708133', 478: '65.06660049298893', 479: '-52.54780137939055', 480: '29.15680639327104', 481: '25.139107420400506', 482: '69.7701311729403', 483: '-11.890002739721297', 484: '-10.124282503068855', 485: '16.763288658672508', 486: '92.5086451434043', 487: '80.18821575370191', 488: '69.143388674135', 489: '-24.206350378240543', 490: '64.64981618764412', 491: '59.32713335079677', 492: '3.642470391625025', 493: '-47.17012341015412', 494: '4.781761764704456', 495: '-79.55403975406591', 496: '-20.69353084650905', 497: '-90.26558330532222', 498: '-65.0890444724145', 499: '13.563917974869398', 500: '-59.935716518278646', 501: '35.85964443850878', 502: '9.23789150881258', 503: '99.23811908175452', 504: '-0.22404945271235022', 505: '89.00116876743783', 506: '96.40934202527953', 507: '-58.93539189585852', 508: '92.0773108837949', 509: '-36.932469569316865', 510: '-44.79379424414489', 511: '-81.09560524869659', 512: '53.044768608758716', 513: '-7.978640119218738', 514: '-7.6561087939104056', 515: '20.505293432183834', 516: '35.77074055908486', 517: '-69.19287947033202', 518: '-53.704545162248095', 519: '-64.2744558653487', 520: '4.805753171098374', 521: '-82.6432129142445', 522: '0.8247077651903254', 523: '-9.239645504031316', 524: '-8.33437980370924', 525: '-10.094598995271141', 526: '-6.2018165161521495', 527: '2.608528502366525', 528: '29.618174271442513', 529: '19.253088507249117', 530: '-59.51620310516381', 531: '-41.55166169814748', 532: '-42.17710561297419', 533: '-69.13672982810515', 534: '74.92404775448554', 535: '19.97652971534191', 536: '-46.46921425337032', 537: '-22.93217504513703', 538: '-20.48815159901416', 539: '-51.053567211192366', 540: '89.59669796171886', 541: '17.54039586114557', 542: '64.67200171180832', 543: '99.13641158688543', 544: '55.079801698340276', 545: '97.93274900340302', 546: '-22.195139730738262', 547: '25.25121942816196', 548: '-64.99447669275295', 549: '-78.50383863844155', 550: '3.8030066181441384', 551: '25.914084421745372', 552: '7.956040815435614', 553: '-42.00446711826549', 554: '72.6134196850393', 555: '-63.187670196217404', 556: '-49.808742583818756', 557: '-77.32873514413727', 558: '0.2376164113768482', 559: '-22.601519696386035', 560: '-89.90711398270581', 561: '54.528072611957924', 562: '-81.94155864726778', 563: '58.89777790240706', 564: '48.92099884326905', 565: '-45.03278827458834', 566: '18.455241250547033', 567: '97.5532161581514', 568: '74.15218077271464', 569: '87.5746614108694', 570: '7.460595965781323', 571: '0.2981618071488299', 572: '-57.570677239974685', 573: '86.00562747142862', 574: '-88.5096394860106', 575: '73.11516213091437', 576: '24.783698906983304', 577: '-97.57085916934783', 578: '-45.068525675976126', 579: '80.46890599346435', 580: '10.107725584062877', 581: '45.150537940260676', 582: '-63.78741653899765', 583: '-24.020612083423387', 584: '54.57431576033835', 585: '-78.2321127849375', 586: '36.85018377610655', 587: '99.920784840393', 588: '70.3959929619987', 589: '87.36132688409768', 590: '20.56445880649808', 591: '-3.376861281006512', 592: '-74.25741223085565', 593: '93.78129320910759', 594: '-94.50714286974629', 595: '-37.221270262741335', 596: '70.38471970700331', 597: '2.1295371784485013', 598: '80.20036632598112', 599: '-57.52946385085873', 600: '46.54883687538256', 601: '87.15111631412168', 602: '-70.22298383603429', 603: '21.542377963736655', 604: '-76.8609043515025', 605: '97.776479547572', 606: '40.68949925950065', 607: '80.54407265397938', 608: '5.454436665855539', 609: '-7.886927335767766', 610: '-14.553627473645818', 611: '-74.60163186586132', 612: '22.2603946419474', 613: '12.218900970419512', 614: '-85.21598769914198', 615: '64.34312350873307', 616: '-94.74885745953638', 617: '-47.13903949544456', 618: '63.276260265034864', 619: '-86.87964260096771', 620: '62.69024075137841', 621: '14.802687073259534', 622: '-8.058475423737548', 623: '19.901476227585135', 624: '-4.40446638815699', 625: '-69.01483330920159', 626: '-28.530816569032908', 627: '-45.79661250070512', 628: '18.03228088693392', 629: '-52.95586562438545', 630: '88.87714509151323', 631: '-52.30170391702866', 632: '50.05773727293635', 633: '-46.45012145758747', 634: '4.78134160771539', 635: '-12.175753770272252', 636: '-7.746756587321755', 637: '-74.78265942571699', 638: '-35.879849140375626', 639: '-0.2736635503598279', 640: '81.8149001947368', 641: '-34.34402231191895', 642: '-44.623186569650585', 643: '-41.269102867775985', 644: '22.448579475910662', 645: '66.04950579806646', 646: '1.593630437971811', 647: '-21.03146636082711', 648: '-55.79191511121384', 649: '-19.674134385441477', 650: '80.2365288799036', 651: '3.9871376850924207', 652: '97.74318270182542', 653: '-86.15598122101', 654: '10.390277752927688', 655: '-92.33961543477483', 656: '-3.4041935307488984', 657: '40.96483567093486', 658: '69.29997498713519', 659: '78.35187072737773', 660: '-35.15149670299742', 661: '96.37658184422992', 662: '-12.152529700991337', 663: '-92.49765994358032', 664: '-95.45711147533203', 665: '52.912080946165844', 666: '-32.58541288791696', 667: '-18.299106228378648', 668: '-56.003805254742886', 669: '-32.45201931145756', 670: '77.00240363634663', 671: '78.0567301890114', 672: '77.38280554403104', 673: '67.73884131887996', 674: '-38.680921849658745', 675: '-7.510495843453981', 676: '24.034852012197263', 677: '67.88765571687685', 678: '23.093347843786688', 679: '-1.987113646169874', 680: '20.929544595578434', 681: '22.02967881518225', 682: '80.98046899086447', 683: '63.126411713180914', 684: '-89.05502153548889', 685: '67.79569220996478', 686: '-73.66280315882916', 687: '47.043517037631204', 688: '72.4709814801113', 689: '-60.441106604315166', 690: '-53.404832270869406', 691: '56.70146971153585', 692: '-42.21146832029001', 693: '1.4939791880674562', 694: '24.757017480218636', 695: '-17.292509736649194', 696: '68.83100403615248', 697: '4.757759779426323', 698: '55.57238093819268', 699: '69.06042426291427', 700: '-54.69297364052288', 701: '86.53879994480326', 702: '-2.7124632600293808', 703: '30.986800542541545', 704: '32.92097621290105', 705: '-86.62533783877491', 706: '-62.3184624927438', 707: '-51.44092765103052', 708: '7.224988403378447', 709: '-81.36979966421873', 710: '-83.49156428683688', 711: '55.179920277381854', 712: '71.61824376133947', 713: '-18.957547299262686', 714: '64.8486298724975', 715: '25.95169705368295', 716: '21.538946335720084', 717: '-17.711405376957373', 718: '-43.921713924574135', 719: '89.57587295532522', 720: '-84.82241226268874', 721: '38.105534354029544', 722: '56.712886865921604', 723: '37.81776465700776', 724: '49.584815720874644', 725: '18.059362205521737', 726: '-66.45057049457972', 727: '-92.61602521013992', 728: '74.11169119297759', 729: '94.43635756796769', 730: '3.0145705319264806', 731: '-15.866237100964526', 732: '-22.502635830080294', 733: '-19.891087939590847', 734: '3.7613046130834675', 735: '-46.182129017367224', 736: '-69.58301787135959', 737: '-10.257628243368686', 738: '20.49768491651649', 739: '-8.437453169276495', 740: '5.5930411157701485', 741: '-43.76747549044413', 742: '7.961013884206622', 743: '-98.93579664165549', 744: '-95.28268826978803', 745: '79.18036870079465', 746: '-58.27828987048047', 747: '-98.13232203282386', 748: '47.69431452398695', 749: '42.053511793236424', 750: '-43.927772589944716', 751: '-46.26147124243734', 752: '-54.926040179870995', 753: '-34.79500437342659', 754: '-15.99778378293793', 755: '29.262831604773368', 756: '94.63756228691793', 757: '26.620465007587214', 758: '35.77490946019273', 759: '18.3935307409931', 760: '1.2847279501498292', 761: '-49.611464162159805', 762: '76.67683002577957', 763: '78.88595075842454', 764: '-9.245759938691904', 765: '28.937389582247874', 766: '-40.44528513637955', 767: '-96.1327535114872', 768: '91.30868959399345', 769: '-54.3013538994241', 770: '-32.206806073456846', 771: '73.68324312746711', 772: '42.349703428959344', 773: '19.393473028985554', 774: '-23.470585095289593', 775: '93.57446582353666', 776: '-23.470672657687118', 777: '-64.31120041170402', 778: '-77.13017591961989', 779: '-17.60825618860318', 780: '-62.94266584457935', 781: '89.97193082564473', 782: '-55.22177789912743', 783: '-90.60929805877234', 784: '67.184891495122', 785: '58.13033529842335', 786: '29.45809450213204', 787: '1.282690498936006', 788: '89.91674411868595', 789: '-93.70883729541465', 790: '89.9533159344854', 791: '-55.8273853362645', 792: '-13.17830368304385', 793: '-32.41491061456408', 794: '-15.794616567307827', 795: '16.788083644938695', 796: '-45.55032878991398', 797: '21.814918548995593', 798: '-96.55723161668092', 799: '-16.058694646488306', 800: '73.17115540810443', 801: '34.518653334657984', 802: '-57.1622025358403', 803: '16.683748294834317', 804: '93.42693696936786', 805: '-92.65300501960789', 806: '-36.71037191550437', 807: '-62.88698124031653', 808: '-43.4837888501701', 809: '79.0137163464772', 810: '74.51651981588705', 811: '-90.64090113834274', 812: '18.940085073269003', 813: '-79.84941398678274', 814: '10.050809516264934', 815: '-1.5392673714480765', 816: '11.4354041261127', 817: '-0.5242283196448909', 818: '86.52392204444212', 819: '60.527858364396906', 820: '89.35151526405593', 821: '-3.812346425384902', 822: '57.406338753732115', 823: '18.424836911698478', 824: '35.17796541583567', 825: '35.554259407801766', 826: '28.41243628788601', 827: '9.836895066394675', 828: '-49.450795878842335', 829: '-48.28789916694258', 830: '-13.294051161356094', 831: '-10.51360322263659', 832: '82.44437855372271', 833: '-95.70244218296529', 834: '85.24243355306473', 835: '-50.64139229915572', 836: '2.660140925411852', 837: '97.75098075498653', 838: '-36.661086386809735', 839: '71.84705520056048', 840: '7.458285239452039', 841: '-60.11702617413772', 842: '-66.74334220412493', 843: '-40.54928168461256', 844: '-55.441270405141175', 845: '35.0147968069804', 846: '-36.75833542959492', 847: '-95.38205936086084', 848: '34.8684701028256', 849: '53.11577698549476', 850: '-55.05653959316912', 851: '-4.418314522679978', 852: '-22.02471147094272', 853: '73.39939692405136', 854: '-91.88512276056187', 855: '61.05387951010826', 856: '-98.33709713961628', 857: '7.277727035190722', 858: '59.15440730750211', 859: '11.983977394030234', 860: '-27.209943362258812', 861: '-13.668948857184901', 862: '1.3430208783962172', 863: '-5.10142639935016', 864: '76.5466075076651', 865: '-3.704428848452963', 866: '-97.18258967953486', 867: '42.91394535954291', 868: '-20.038637844382706', 869: '-91.8121736498656', 870: '-27.243633456271453', 871: '41.69431605262167', 872: '-39.29746892474783', 873: '62.05447304921776', 874: '-65.24175435563404', 875: '68.14520662211166', 876: '15.633464647392813', 877: '-36.921528019529305', 878: '-66.15590767894452', 879: '44.312041502456054', 880: '-84.0887882814979', 881: '-83.37075421870239', 882: '-74.87883770819785', 883: '19.130186816956126', 884: '-43.12712341692302', 885: '-58.53479515449456', 886: '-92.2998927145035', 887: '-20.452837910282454', 888: '-14.58435174171504', 889: '-27.066671259103913', 890: '7.8236578163576525', 891: '-70.26625964863624', 892: '78.88849708407204', 893: '-42.8132707549842', 894: '-64.96611969975964', 895: '-77.78062932804187', 896: '0.9416197743541375', 897: '73.55601960720409', 898: '79.4553479377528', 899: '98.09879014325716', 900: '62.98243817615716', 901: '42.571232008966376', 902: '-20.217830151907094', 903: '-2.36756190505443', 904: '87.90815419253664', 905: '43.58219452156678', 906: '19.843047842534716', 907: '25.783194898729803', 908: '-27.36012001782177', 909: '-82.69447744327545', 910: '97.65161102674276', 911: '54.195300293214956', 912: '94.49779837239902', 913: '-38.56023068042482', 914: '-86.27193461894828', 915: '37.05035363968463', 916: '-43.57786671439094', 917: '-63.08102372429829', 918: '41.83324849119222', 919: '9.06134073859701', 920: '-29.44329950689813', 921: '44.51491433301916', 922: '92.70772420184159', 923: '-26.901340352205082', 924: '-33.38423064316987', 925: '-47.80888491677862', 926: '-96.72841807266592', 927: '-14.571675698612282', 928: '-29.13231463970358', 929: '24.74146396257892', 930: '42.24248323732246', 931: '94.84531799765932', 932: '-73.7056150541481', 933: '-55.566416142547936', 934: '81.87535362843721', 935: '54.67538181190719', 936: '1.7919962727859655', 937: '60.29271089725495', 938: '-44.64174123082023', 939: '67.84741526555896', 940: '14.288666856138477', 941: '87.22085595207932', 942: '-12.133812165638489', 943: '86.67346890865142', 944: '-97.11332448798669', 945: '9.043617840544556', 946: '-3.074525437362979', 947: '-53.83928065105712', 948: '94.54297010092927', 949: '7.955359199674916', 950: '59.07240499688331', 951: '1.5147600529072065', 952: '-98.48866695358491', 953: '-68.62556605786112', 954: '51.11769696993079', 955: '-89.12738833900934', 956: '-27.598551285536516', 957: '-6.980990204990107', 958: '-91.88826334809295', 959: '-49.03264057031578', 960: '-38.74938686085183', 961: '86.07106124518907', 962: '4.911631857198472', 963: '-27.877581446103235', 964: '20.064698357528044', 965: '17.154843565398764', 966: '-21.41563240695325', 967: '38.86816339219948', 968: '86.54385662282692', 969: '-48.44745034633227', 970: '40.93016597784617', 971: '-84.76775140847346', 972: '31.124951621755656', 973: '-87.86982573926771', 974: '-47.80126472461863', 975: '-23.40183293488458', 976: '28.558983829707188', 977: '92.26599214344202', 978: '54.55112895342026', 979: '38.53350679593649', 980: '-65.13724460617064', 981: '45.33301004296854', 982: '-43.32308172505428', 983: '64.3395163669482', 984: '-74.97943781441023', 985: '45.05169628902351', 986: '-8.72311040226235', 987: '-5.010442372454932', 988: '98.83480594507392', 989: '65.0304421663528', 990: '-48.806238785954534', 991: '24.79111432770705', 992: '-28.482665364192613', 993: '-59.41742299471393', 994: '-78.7628060093658', 995: '58.17490732480013', 996: '68.21519197942311', 997: '-31.455936783156815', 998: '-45.34753163379654', 999: '59.58472371564906', 1000: '73.93610212789861', 1001: '36.18559886167543', 1002: '-93.22864049159595', 1003: '-31.022771628038328', 1004: '-38.35153670370399', 1005: '17.46897773686061', 1006: '47.253223835516565', 1007: '33.93468490042909', 1008: '-54.61454525127054', 1009: '55.281683520261254', 1010: '79.22848318895254', 1011: '-58.35527098199744', 1012: '-48.27954862207247', 1013: '95.60519798257795', 1014: '58.22991267600969', 1015: '-57.688170633504484', 1016: '81.55028628940786', 1017: '-87.53510298688532', 1018: '-9.930368415684072', 1019: '82.23471685042648', 1020: '-88.61136578252197', 1021: '-97.3432943990036', 1022: '1.1755178105282198', 1023: '36.68494440297343', 1024: '-15.522464957447184', 1025: '78.042013822826', 1026: '55.01728117633243', 1027: '-57.0518664804039', 1028: '25.854953499319038', 1029: '-38.732556747254264', 1030: '11.415880019022339', 1031: '-22.903649458667303', 1032: '49.73218592796596', 1033: '20.80525612797905', 1034: '67.19635607899639', 1035: '-95.07250735727601', 1036: '39.512466997313', 1037: '-92.21217219167077', 1038: '14.996920303840085', 1039: '-17.170299842859563', 1040: '79.26050032237941', 1041: '-12.960429796107718', 1042: '65.23106883138414', 1043: '-35.182511826769684', 1044: '10.270489930983203', 1045: '-89.3129688531811', 1046: '17.735333176093775', 1047: '-28.872673457588277', 1048: '48.136726207043665', 1049: '-87.80146060832246', 1050: '32.90685904013978', 1051: '-23.28283286605432', 1052: '-1.455680880716855', 1053: '-26.01406807329616', 1054: '30.805763732124348', 1055: '89.9288426144274', 1056: '3.865971136127257', 1057: '-72.83581905066447', 1058: '19.320197500968675', 1059: '-55.89486631547342', 1060: '-11.65747884089076', 1061: '7.169523079252272', 1062: '89.67683030005108', 1063: '-74.72631676148514', 1064: '77.4379532453809', 1065: '87.01186746197521', 1066: '-97.60497365259657', 1067: '93.67529799068632', 1068: '-1.4572671595879996', 1069: '23.960800455158534', 1070: '-1.763243970671354', 1071: '-33.66393971688339', 1072: '74.8602420240353', 1073: '-34.869462044132746', 1074: '-84.8347263036217', 1075: '65.20696590991344', 1076: '-58.57107792631242', 1077: '31.112221078302497', 1078: '13.675205019353754', 1079: '17.2961452543626', 1080: '-74.28027887302717', 1081: '-81.31233543972307', 1082: '37.05942396123967', 1083: '-2.188851441082676', 1084: '74.72813298144536', 1085: '-38.21636698706645', 1086: '-63.635621109536046', 1087: '74.98720854770434', 1088: '5.908639238656789', 1089: '-38.83996119680333', 1090: '-7.6979905610990755', 1091: '-55.65475035786096', 1092: '-21.07307350092246', 1093: '-62.336915124865435', 1094: '90.35768495558392', 1095: '42.367081991311125', 1096: '-61.10495471449167', 1097: '38.09423051653863', 1098: '-1.7310215198378103', 1099: '57.68045165711729', 1100: '-78.86914102542855', 1101: '-24.650247107552886', 1102: '25.78577441362728', 1103: '90.58812890324671', 1104: '-2.7561995769580534', 1105: '38.32009647274316', 1106: '-21.57609861826664', 1107: '-34.53888064834314', 1108: '80.55739360925406', 1109: '9.745786215205968', 1110: '75.02182175676714', 1111: '34.88172024320835', 1112: '-0.6873749386942194', 1113: '62.70669379289103', 1114: '-6.358245645339707', 1115: '90.94359106353409', 1116: '83.55659957270717', 1117: '40.3988273840034', 1118: '73.79282532403698', 1119: '-10.200337695217243', 1120: '62.66302660828114', 1121: '39.66154945645721', 1122: '75.89545196863443', 1123: '74.73072996332951', 1124: '-60.69575695409297', 1125: '-43.20032838758821', 1126: '-10.282423033412186', 1127: '-83.41781635903996', 1128: '-4.963447090549252', 1129: '86.47824770592078', 1130: '-89.68819944016994', 1131: '2.501429239175735', 1132: '-67.11095157954438'}\n"
     ]
    }
   ],
   "source": [
    "#copy\n",
    "from collections import Counter\n",
    "\n",
    "class Vocabulary():\n",
    "    END_OF_SENTENCE = '<end>'\n",
    "    NULL = 'NULL'\n",
    "    UNKNOWN = 'UNK'\n",
    "    END_OF_SENTENCE_INDEX = 2\n",
    "    def __init__(self):\n",
    "        self.tok2ind = {self.NULL: 0, self.UNKNOWN: 1, self.END_OF_SENTENCE: 2}\n",
    "        self.ind2tok = {0: self.NULL, 1: self.UNKNOWN, 2: self.END_OF_SENTENCE}\n",
    "    \n",
    "    def add(self, token):\n",
    "        if token not in self.tok2ind:\n",
    "            index = len(self.tok2ind)\n",
    "            self.tok2ind[token] = index\n",
    "            self.ind2tok[index] = token\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tok2ind)\n",
    "    \n",
    "    def get_index(self, word):\n",
    "        if word in self.tok2ind:\n",
    "            return self.tok2ind[word]\n",
    "        return self.tok2ind[self.UNKNOWN]\n",
    "    \n",
    "    def get_word(self, i):\n",
    "        return self.ind2tok[i]\n",
    "\n",
    "    def sentence_to_indices(self, sentence):\n",
    "        words = [x for x in sentence.split(' ')]\n",
    "        words.append(self.END_OF_SENTENCE)\n",
    "        indices = [self.get_index(w) for w in words]\n",
    "        return indices\n",
    "\n",
    "def build_vocab(examples):\n",
    "    counts = Counter()\n",
    "    for ex in examples:\n",
    "        words = [w for w in ex.split(' ') if w.strip()]\n",
    "        counts.update(words)\n",
    "    \n",
    "    word_list = [w for w in counts if counts[w] > 1]\n",
    "\n",
    "    word_dict = Vocabulary()\n",
    "    for w in word_list:\n",
    "        word_dict.add(w)\n",
    "    return word_dict\n",
    "\n",
    "input_vocab = build_vocab(train_x)\n",
    "output_vocab = build_vocab(train_y)\n",
    "print(input_vocab.tok2ind)\n",
    "print(output_vocab.ind2tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# class Vocabulary():\n",
    "#     END_OF_SENTENCE = '<end>'\n",
    "#     NULL = 'NULL'\n",
    "#     UNKNOWN = 'UNK'\n",
    "#     END_OF_SENTENCE_INDEX = 2\n",
    "#     def __init__(self):\n",
    "#         self.tok2ind = {self.NULL: 0, self.UNKNOWN: 1, self.END_OF_SENTENCE: 2}\n",
    "#         self.ind2tok = {0: self.NULL, 1: self.UNKNOWN, 2: self.END_OF_SENTENCE}\n",
    "    \n",
    "#     def add(self, token):\n",
    "#         if token not in self.tok2ind:\n",
    "#             index = len(self.tok2ind)\n",
    "#             self.tok2ind[token] = index\n",
    "#             self.ind2tok[index] = token\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.tok2ind)\n",
    "    \n",
    "#     def get_index(self, word):\n",
    "#         if word in self.tok2ind:\n",
    "#             return self.tok2ind[word]\n",
    "#         return self.tok2ind[self.UNKNOWN]\n",
    "    \n",
    "#     def get_word(self, i):\n",
    "#         return self.ind2tok[i]\n",
    "\n",
    "#     def sentence_to_indices(self, sentence):\n",
    "#         words = [x for x in sentence.split(' ')]\n",
    "#         words.append(self.END_OF_SENTENCE)\n",
    "#         indices = [self.get_index(w) for w in words]\n",
    "#         return indices\n",
    "    \n",
    "# import string\n",
    "\n",
    "# def replace_punct(sentence):\n",
    "#     \"\"\"\n",
    "#     Replaces with punctuation with a space + punctuation\n",
    "#     \"\"\"\n",
    "#     result = \"\"\n",
    "#     for c in (sentence.lower()):\n",
    "#         if (c != '(' and c in string.punctuation):\n",
    "#             result += \" \" \n",
    "#         result += c\n",
    "        \n",
    "#     return result\n",
    "\n",
    "# def build_vocab(examples):\n",
    "#     counts = Counter()\n",
    "#     for ex in examples:\n",
    "# #         curr  = replace_punct(ex)\n",
    "\n",
    "#         words = [w for w in curr.split(' ') if w.strip()]\n",
    "#         counts.update(words)\n",
    "    \n",
    "#     word_list = [w for w in counts if counts[w] > 1]\n",
    "# #     for i in range(len(word_list)):\n",
    "# #         for k, v in copy_mappings.items():\n",
    "# #             if word_list[i] in v:\n",
    "# #                 word_list[i] = k\n",
    "# #                 break\n",
    "#     word_dict = Vocabulary()\n",
    "#     for w in word_list:\n",
    "#         word_dict.add(w)\n",
    "#     return word_dict\n",
    "\n",
    "# input_vocab = build_vocab(train_x)\n",
    "# output_vocab = build_vocab(train_y)\n",
    "# # print(train_y)\n",
    "# print(input_vocab.tok2ind)\n",
    "# print(output_vocab.tok2ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import Sampler\n",
    "import string\n",
    "\n",
    "class Example():\n",
    "    def __init__(self, x_str, y_str, input_vocab, output_vocab):\n",
    "        self.x_str = x_str\n",
    "        self.y_str = y_str\n",
    "        self.x_toks = x_str.split(' ')\n",
    "#         for i in range(len(self.x_toks)):\n",
    "#             for k, v in copy_dict.items():\n",
    "#                 if self.x_toks[i] in v:\n",
    "#                     self.x_toks[i] = k\n",
    "        \n",
    "        self.y_toks = y_str.split(' ')\n",
    "        self.input_vocab = input_vocab\n",
    "        self.output_vocab = output_vocab\n",
    "        self.x_inds = torch.LongTensor(input_vocab.sentence_to_indices(x_str))\n",
    "        self.y_inds = torch.LongTensor(output_vocab.sentence_to_indices(y_str))\n",
    "        \n",
    "        # for copying\n",
    "        self.y_in_x_inds = torch.FloatTensor(([[int(x_tok == y_tok) for x_tok in self.x_toks] for y_tok in self.y_toks])) \n",
    "#         print(\"x\", self.x_toks, self.y_toks)\n",
    "\n",
    "# In order to use PyTorch's data loader\n",
    "class ReaderDataset(Dataset):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.examples[index]\n",
    "    \n",
    "train_exs = []\n",
    "\n",
    "for x,y in zip(train_x, train_y):\n",
    "    train_exs.append(Example(x, y, input_vocab, output_vocab))\n",
    "train_dataset = ReaderDataset(train_exs)\n",
    "\n",
    "test_exs = []\n",
    "for x,y in zip(test_x, test_y):\n",
    "    test_exs.append(Example(x, y, input_vocab, output_vocab))\n",
    "\n",
    "test_dataset = ReaderDataset(test_exs)\n",
    "print([ex.input_vocab.tok2ind for ex in test_dataset.examples])\n",
    "\n",
    "# for x in test_dataset:\n",
    "#     print(x.y_in_x_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize individual examples and organize them into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize batch data\n",
    "def vectorize(batch):\n",
    "    max_input_length = max([ex.x_inds.size(0) for ex in batch])\n",
    "    x = torch.LongTensor(len(batch), max_input_length).zero_() # initialize to 0\n",
    "    x_mask = torch.ByteTensor(len(batch), max_input_length).fill_(1) # mask used in softmax\n",
    "    x_lens = torch.LongTensor(len(batch)).zero_()\n",
    "    for i, ex in enumerate(batch):\n",
    "        x[i, :ex.x_inds.size(0)].copy_(ex.x_inds)\n",
    "        x_mask[i, :ex.x_inds.size(0)].fill_(0)\n",
    "        ###CHANGE: x_lens store the last index of each sequence. i*max_input_length is added so that later we can use \n",
    "        ###torch.index_select to get the last hidden states from a 2D tensor (batch_size*max_input_length, embedding_dim)\n",
    "        x_lens[i] = i*max_input_length+ex.x_inds.size(0)-1 \n",
    "    \n",
    "    max_output_length = max([ex.y_inds.size(0) for ex in batch])\n",
    "    y = torch.LongTensor(len(batch), max_output_length).zero_()\n",
    "    y_mask = torch.ByteTensor(len(batch), max_output_length).zero_() # for masked_select\n",
    "    for i, ex in enumerate(batch):\n",
    "        y[i, :ex.y_inds.size(0)].copy_(ex.y_inds)\n",
    "        y_mask[i, :ex.y_inds.size(0)].fill_(1)\n",
    "    \n",
    "    # for copying\n",
    "    y_in_x_inds = torch.FloatTensor(len(batch), max_output_length, max_input_length).zero_()\n",
    "    for i, ex in enumerate(batch):\n",
    "        y_in_x_inds[i, :ex.y_in_x_inds.size(0), :ex.y_in_x_inds.size(1)].copy_(ex.y_in_x_inds)\n",
    "\n",
    "    return x, x_lens, x_mask, y, y_mask, y_in_x_inds\n",
    "\n",
    "train_sampler = torch.utils.data.sampler.RandomSampler(train_dataset)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=100, ## the batch_size can be tuned\n",
    "    sampler=train_sampler,\n",
    "    num_workers=1,\n",
    "    collate_fn=vectorize\n",
    ")\n",
    "\n",
    "test_sampler = torch.utils.data.sampler.SequentialSampler(test_dataset)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1, ## the batch_size can be tuned\n",
    "    sampler=test_sampler,\n",
    "    num_workers=1,\n",
    "    collate_fn=vectorize\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Build the seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack bidirectional LSTM\n",
    "class StackBRNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=1):\n",
    "        super(StackBRNN, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.rnns = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            input_dim = input_dim if i == 0 else hidden_dim * 2\n",
    "            self.rnns.append(nn.LSTM(input_dim, hidden_dim, bidirectional=True))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Transpose batch and sequence dims\n",
    "        x = x.transpose(0, 1) # (seq_len, batch_size, input_dim)\n",
    "\n",
    "        outputs = [x]\n",
    "        for i in range(self.num_layers):\n",
    "            rnn_input = outputs[-1]\n",
    "            rnn_output = self.rnns[i](rnn_input)[0]\n",
    "            outputs.append(rnn_output)\n",
    "\n",
    "        h_output = outputs[-1]\n",
    "\n",
    "        # Transpose back\n",
    "        h_output = h_output.transpose(0, 1) # (batch_size, seq_len, 2*hidden_dim)\n",
    "        \n",
    "        return h_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2.1: Define the basic seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, input_vocab, output_vocab, copying=False):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.input_vocab = input_vocab\n",
    "        self.output_vocab = output_vocab\n",
    "        self.in_vocab_size = len(self.input_vocab)\n",
    "        self.out_vocab_size = len(self.output_vocab)\n",
    "        \n",
    "        self.in_embedding = nn.Embedding(self.in_vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.encoder = StackBRNN(embedding_dim, hidden_dim)\n",
    "        \n",
    "        self.out_embedding = nn.Embedding(self.out_vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        #Inputs: input, (h_0, c_0)\n",
    "        #Outputs: h_1, c_1\n",
    "        self.decoder = nn.LSTMCell(embedding_dim, hidden_dim) \n",
    "         \n",
    "        self.enc_to_dec = nn.Linear(hidden_dim*2, hidden_dim) # project encoding outupt\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_dim, self.out_vocab_size)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x_emb = self.in_embedding(x)\n",
    "        output = self.encoder(x_emb) # output: (batch_size, seq_len, hidden_dim*2)\n",
    "        return output\n",
    "    \n",
    "    def decode(self, h_prev):\n",
    "        out = self.output_layer(h_prev[0])\n",
    "        probs = F.softmax(out, dim=1)\n",
    "        \n",
    "        return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attention.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSeq2Seq(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, input_vocab, output_vocab, copying=False):\n",
    "        super(AttentionSeq2Seq, self).__init__()\n",
    "        self.input_vocab = input_vocab\n",
    "        self.output_vocab = output_vocab\n",
    "        self.in_vocab_size = len(self.input_vocab)\n",
    "        self.out_vocab_size = len(self.output_vocab)\n",
    "        self.copying = copying\n",
    "        \n",
    "        self.in_embedding = nn.Embedding(self.in_vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.encoder = StackBRNN(embedding_dim, hidden_dim)\n",
    "        \n",
    "        self.out_embedding = nn.Embedding(self.out_vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        #Inputs: input, (h_0, c_0)\n",
    "        #Outputs: h_1, c_1\n",
    "        self.decoder = nn.LSTMCell(embedding_dim + hidden_dim*2, hidden_dim) # concatenate y_t and context_t\n",
    "        \n",
    "        self.enc_to_dec = nn.Linear(hidden_dim*2, hidden_dim) # project encoding outupt\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_dim + hidden_dim*2, self.out_vocab_size) # concatenate h_t and context_t\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x_emb = self.in_embedding(x) #(batch_size, seq_len, embedding_dim)\n",
    "        # map this part to glove\n",
    "        output = self.encoder(x_emb) # output: (batch_size, seq_len, hidden_dim*2)\n",
    "        return output\n",
    "    \n",
    "    def decode(self, encoder_outputs, encoder_proj_outputs, x_mask, h_prev):\n",
    "        # (batch_size, seq_len, hidden_dim) * (batch_size, hidden_dim, 1) - >(batch_size, seq_len, 1)\n",
    "        scores = torch.bmm(encoder_proj_outputs, h_prev[0].unsqueeze(2)).squeeze(2) # scores: (batch_size, seq_len)\n",
    "        scores.data.masked_fill_(x_mask.data, -float('inf'))\n",
    "        alpha = F.softmax(scores, dim=1)\n",
    "        # (batch_size, 1, seq_len) * (batch_size, seq_len, hidden_dim) - > (batch_size, 1, hidden_dim)\n",
    "        context_t = torch.bmm(alpha.unsqueeze(1), encoder_outputs).squeeze(1) # context_t: (batch_size, hidden_dim) \n",
    "        \n",
    "        out = self.output_layer(torch.cat([h_prev[0], context_t], 1))\n",
    "        \n",
    "        if self.copying: \n",
    "            probs = F.softmax(torch.cat([out, scores], 1), dim=1) # Appending scores over the input\n",
    "        else:\n",
    "            probs = F.softmax(out, dim=1)\n",
    "    \n",
    "        return probs, context_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2.2: Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize and train the network:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(ex, model, optim):\n",
    "    model.train()\n",
    "    \n",
    "    x, x_lens, x_mask, y, y_mask, y_in_x_inds = ex\n",
    "    \n",
    "    # Variable(x.cuda()) if using GPU\n",
    "    x, x_lens, x_mask, y, y_mask, y_in_x_inds = Variable(x), Variable(x_lens), Variable(x_mask), Variable(y), Variable(y_mask), Variable(y_in_x_inds)\n",
    "    \n",
    "    encoder_outputs = model.encode(x) # (batch_size, seq_len, hidden_dim*2)\n",
    "    encoder_proj_outputs = model.enc_to_dec(encoder_outputs) # (batch_size, seq_len, hidden_dim)\n",
    "    \n",
    "    ###CHANGE: make use of x_lens to index the last hidden states\n",
    "    batch_size = x.size(0)\n",
    "    seq_len = x.size(1)\n",
    "    h_0 = torch.index_select(encoder_proj_outputs.view(batch_size*seq_len,-1),0,x_lens) # be careful when input sequences have paddings\n",
    "    \n",
    "    c_0 = Variable(torch.zeros(h_0.size(0), h_0.size(1)).zero_()) \n",
    "    hidden = (h_0, c_0)\n",
    "    \n",
    "    p_y_seq = []\n",
    "    for i in range(y.size(1)):\n",
    "        #output = model.decode(hidden) \n",
    "        #y_emb = model.out_embedding(y[:, i]) # y_emb: (batch_size, embedding_dim)        \n",
    "        #hidden = model.decoder(y_emb, hidden) # (h_t, c_t): (batch_size, hidden_dim)\n",
    "        \n",
    "        ###CHANGE: update the decode function, move the code that uses y[:, i] down\n",
    "        output, context_t = model.decode(encoder_outputs, encoder_proj_outputs, x_mask, hidden) # with attention\n",
    "        \n",
    "        ###compute the next hidden state using the current output y[:, i]\n",
    "        y_emb = model.out_embedding(y[:, i]) # y_emb: (batch_size, embedding_dim)\n",
    "        hidden = model.decoder(torch.cat([y_emb, context_t], 1), hidden) \n",
    "        \n",
    "        p_y_t = output.gather(1, y[:, i].unsqueeze(1)) # (batch_size, 1)\n",
    "        \n",
    "        if model.copying:\n",
    "            copy_dist = output[:, model.out_vocab_size:model.out_vocab_size + y_in_x_inds.size(2)] # (batch_size, input_len)\n",
    "            # (batch_size, 1, input_len), (batch_size, input_len, 1)\n",
    "            copying_p_y_t = torch.bmm(copy_dist.unsqueeze(1), y_in_x_inds[:, i].unsqueeze(2)).squeeze(2)\n",
    "            p_y_t = p_y_t + copying_p_y_t\n",
    "                \n",
    "        p_y_seq.append(p_y_t)\n",
    "\n",
    "    p_y_seq = torch.cat([_ for _ in p_y_seq], 1) # (batch_size, seq_len)\n",
    "    p_y_seq = torch.masked_select(p_y_seq, y_mask)\n",
    "    loss = -torch.sum(torch.log(p_y_seq))/y.size(0) # loss = -\\sum_i log p(y|x)\n",
    "\n",
    "    # Clear gradients and run backward\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients, max_norm * v/||v|| if ||v|| > max_norm\n",
    "    torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=10.0)\n",
    "\n",
    "    # Update parameters\n",
    "    optim.step()\n",
    "    \n",
    "    return loss.data[0]\n",
    "\n",
    "#model = Seq2Seq(50, 20, input_vocab, output_vocab)\n",
    "model = AttentionSeq2Seq(50, 20, input_vocab, output_vocab, True)\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "# training loop\n",
    "n_epochs = 150\n",
    "for e in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "    for ex in train_loader:\n",
    "        l = train(ex, model, optim)\n",
    "        train_loss += l\n",
    "    print(\"Epoch = %d | Loss = %.2f\" % (e, train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2.3: Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model, similar to training. Using greedy search to infer the most likely sequence output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_batch(data_loader, model, max_len=15):\n",
    "    model.eval()\n",
    "    \n",
    "    num_correct = 0\n",
    "    for ex in data_loader:\n",
    "        x, x_lens, x_mask, y, y_mask, y_in_x_inds = ex \n",
    "        \n",
    "        x, x_lens, x_mask = Variable(x), Variable(x_lens), Variable(x_mask)\n",
    "    \n",
    "        encoder_outputs = model.encode(x) # (batch_size, seq_len, hidden_dim*2)\n",
    "        encoder_proj_outputs = model.enc_to_dec(encoder_outputs) # (batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        ###CHANGE: make use of x_lens to index the last hidden states\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "        h_0 = torch.index_select(encoder_proj_outputs.view(batch_size*seq_len,-1),0,x_lens) # be careful when input sequences have paddings\n",
    "    \n",
    "        c_0 = Variable(torch.zeros(h_0.size(0), h_0.size(1)).zero_()) \n",
    "        hidden = (h_0, c_0)\n",
    "        \n",
    "        ###CHANGE: start with empty prediction\n",
    "        seq = []\n",
    "        for i in range(max_len):\n",
    "            #output = model.decode(hidden) \n",
    "            \n",
    "            ###CHANGE: update the decode function, move the code that uses y[:, i] down\n",
    "            output, context_t = model.decode(encoder_outputs, encoder_proj_outputs, x_mask, hidden) # with attention\n",
    "        \n",
    "            sampleLogprobs, it = torch.max(output.data, 1)\n",
    "            y_t = it.view(-1).long()\n",
    "            seq.append(y_t)\n",
    "            \n",
    "            if model.copying:\n",
    "                new_y_t = []\n",
    "                for j in range(y_t.size(0)):\n",
    "                    if y_t[j] < model.out_vocab_size:\n",
    "                        new_y_t.append(y_t[j])\n",
    "                    else:\n",
    "                        k = x.data[j, y_t[j]-model.out_vocab_size]\n",
    "                        w = model.input_vocab.get_word(k)\n",
    "                        new_k = model.output_vocab.get_index(w)\n",
    "                        new_y_t.append(new_k)\n",
    "                y_t = torch.LongTensor(new_y_t)\n",
    "            \n",
    "            ###compute the next hidden state using the current output y_t\n",
    "            y_prev = Variable(y_t)\n",
    "            y_emb = model.out_embedding(y_prev) # y_emb: (batch_size, embedding_dim)\n",
    "            hidden = model.decoder(torch.cat([y_emb, context_t], 1), hidden) \n",
    "            \n",
    "            #hidden = model.decoder(y_emb, hidden)\n",
    "        \n",
    "        pred_y = torch.cat([_.unsqueeze(1) for _ in seq], 1)\n",
    "        \n",
    "        for idx in range(batch_size):\n",
    "            gold_toks = []\n",
    "            for wi in y[idx].tolist():\n",
    "                gold_toks.append(model.output_vocab.get_word(wi))\n",
    "            print(\"Gold: \", ' '.join(gold_toks))\n",
    "        \n",
    "            pred_toks = []\n",
    "            for wi in pred_y[idx].tolist():\n",
    "                #w = model.output_vocab.get_word(wi)\n",
    "            \n",
    "                if wi < model.out_vocab_size:\n",
    "                    w = model.output_vocab.get_word(wi)\n",
    "                else:\n",
    "                    w = model.input_vocab.get_word(x.data[idx][wi-model.out_vocab_size])\n",
    "                    #print(\"copying \", w)\n",
    "                pred_toks.append(w)\n",
    "                if w == \"<end>\":\n",
    "                    break\n",
    "                \n",
    "            print(\"Predict: \",' '.join(pred_toks))\n",
    "            \n",
    "            if gold_toks == pred_toks:\n",
    "                num_correct += 1\n",
    "                    \n",
    "                    \n",
    "    print(\"Test accuracy: {}\".format(num_correct / len(data_loader)))\n",
    "\n",
    "    \n",
    "print(test_loader.)\n",
    "        \n",
    "test_batch(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ex.x_str for ex in test_loader.dataset.examples]\n",
    "# print(test_loader.dataset.examples[1].x_str)\n",
    "# test_loader.dataset.examples[1].input_vocab.tok2ind"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
