{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7eff54ed7290>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1013)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/codegen.csv\")\n",
    "data_x = df[\"utterance\"]\n",
    "data_y = df[\"targets\"]\n",
    "\n",
    "\"\"\"\n",
    "# import dataset\n",
    "with open(\"calculator.dataset\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "data_x, data_y = [], []\n",
    "for line in lines:\n",
    "    if (line[0] == \"(\"):\n",
    "        data_y.append(line.strip())\n",
    "    elif (line != \"\\n\"):\n",
    "        data_x.append(line.strip())\n",
    "\"\"\"\n",
    "# split into test/train data\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size=0.2)\n",
    "\n",
    "test_x.loc[38] = \"What is the minimum humidity?\"\n",
    "test_y.loc[38] = \"min( WeatherHistory [ 'Humidity' ] )\"\n",
    "\n",
    "test_x.loc[39] = \"What is the lowest humidity?\"\n",
    "test_y.loc[39] = \"min( WeatherHistory [ 'Humidity' ] )\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building input and output vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NULL': 0, 'UNK': 1, '<end>': 2, 'What': 3, 'is': 4, 'the': 5, 'average': 6, 'temperature': 7, 'of': 8, 'Weather': 9, 'History': 10, 'dataset?': 11, 'Of': 12, 'listed': 13, 'values,': 14, 'which': 15, 'to': 16, 'how': 17, 'it': 18, \"what's\": 19, 'mean': 20, 'temperature?': 21, 'minimum': 22, 'linear': 23, 'relationship': 24, '_': 25, 'and': 26, '_?': 27, 'What’s': 28, 'coldest': 29, 'today?': 30, 'humidity': 31, 'when': 32, '12?': 33, 'Give': 34, 'maximum': 35, 'recorded': 36, 'data': 37, 'WeatherHistory.': 38, 'How': 39, 'are': 40, 'humidity?': 41, 'Tell': 42, 'me': 43, 'what': 44, 'is.': 45, '12': 46, 'predict': 47, 'value': 48, 'Find': 49, 'temperature.': 50, 'correlation': 51, 'between': 52, 'feeling': 53, \"What's\": 54, 'regression': 55, 'model': 56, 'predicting': 57, 'entry': 58, 'in': 59, 'did': 60, 'get': 61, 'based': 62, 'on': 63, 'Can': 64, 'you': 65, 'predicted': 66, 'values': 67, 'for': 68, 'described': 69, 'previous': 70, 'degrees': 71, 'celsius?': 72, 'a': 73, 'temperatures': 74, 'WeatherHistory?': 75, 'tell': 76, 'was': 77, 'day?': 78, \"it's\": 79, 'ever': 80, 'any': 81, 'higher': 82, 'than': 83, 'actual': 84, '(and': 85, 'vice-versa)?': 86, 'give': 87, 'highest': 88, 'variables': 89, 'Predict': 90, 'level': 91, '12.': 92, 'your': 93, 'prediction': 94, 'lowest': 95, 'value.': 96, 'will': 97, 'be': 98, 'recorded?': 99, 'at': 100, 'degrees.': 101, 'there': 102, 'feeling_temperature?': 103, 'degree.': 104, 'value?': 105, 'extent': 106, 'change': 107, 'hot': 108, 'does': 109, 'At': 110, 'degrees,': 111, 'weather': 112, 'history?': 113, 'be?': 114, 'hottest': 115, 'forecasted': 116, 'Please': 117, 'find': 118, 'history': 119, 'base.': 120, 'b/w': 121, 'can': 122, 'degrees?': 123, 'like': 124}\n",
      "{'NULL': 0, 'UNK': 1, '<end>': 2, 'mean(': 3, 'WeatherHistory': 4, '[': 5, \"'Temperature'\": 6, ']': 7, ')': 8, 'max(': 9, 'cor(': 10, 'WeatherHistroy': 11, ',': 12, \"'FellingTemperature'\": 13, 'lm(': 14, \"'Humidity'\": 15, 'min(': 16, 'predict(': 17, 'mod': 18, '12': 19}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class Vocabulary():\n",
    "    END_OF_SENTENCE = '<end>'\n",
    "    NULL = 'NULL'\n",
    "    UNKNOWN = 'UNK'\n",
    "    END_OF_SENTENCE_INDEX = 2\n",
    "    def __init__(self):\n",
    "        self.tok2ind = {self.NULL: 0, self.UNKNOWN: 1, self.END_OF_SENTENCE: 2}\n",
    "        self.ind2tok = {0: self.NULL, 1: self.UNKNOWN, 2: self.END_OF_SENTENCE}\n",
    "    \n",
    "    def add(self, token):\n",
    "        if token not in self.tok2ind:\n",
    "            index = len(self.tok2ind)\n",
    "            self.tok2ind[token] = index\n",
    "            self.ind2tok[index] = token\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tok2ind)\n",
    "    \n",
    "    def get_index(self, word):\n",
    "        if word in self.tok2ind:\n",
    "            return self.tok2ind[word]\n",
    "        return self.tok2ind[self.UNKNOWN]\n",
    "    \n",
    "    def get_word(self, i):\n",
    "        return self.ind2tok[i]\n",
    "\n",
    "    def sentence_to_indices(self, sentence):\n",
    "        words = [x for x in sentence.split(' ')]\n",
    "        words.append(self.END_OF_SENTENCE)\n",
    "        indices = [self.get_index(w) for w in words]\n",
    "        return indices\n",
    "\n",
    "def build_vocab(examples):\n",
    "    counts = Counter()\n",
    "    for ex in examples:\n",
    "        words = [w for w in ex.split(' ') if w.strip()]\n",
    "        counts.update(words)\n",
    "    \n",
    "    word_list = [w for w in counts if counts[w] > 1]\n",
    "    \n",
    "    word_dict = Vocabulary()\n",
    "    for w in word_list:\n",
    "        word_dict.add(w)\n",
    "    return word_dict\n",
    "\n",
    "input_vocab = build_vocab(train_x)\n",
    "output_vocab = build_vocab(train_y)\n",
    "print(input_vocab.tok2ind)\n",
    "print(output_vocab.tok2ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 11x7]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 11x8]\n",
      "\n",
      "\n",
      "\n",
      "Columns 0 to 12 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 13 to 16 \n",
      "    0     0     0     0\n",
      "    0     0     0     0\n",
      "    0     0     0     0\n",
      "    0     0     0     0\n",
      "    0     0     0     0\n",
      "    0     0     0     0\n",
      "    0     0     0     0\n",
      "    0     0     0     0\n",
      "    0     0     0     0\n",
      "    0     0     0     0\n",
      "    0     0     0     0\n",
      "[torch.FloatTensor of size 11x17]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 11x8]\n",
      "\n",
      "\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      "[torch.FloatTensor of size 6x6]\n",
      "\n",
      "\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      "[torch.FloatTensor of size 6x6]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 6x11]\n",
      "\n",
      "\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      "[torch.FloatTensor of size 6x5]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     1     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 5x12]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 11x10]\n",
      "\n",
      "\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      "[torch.FloatTensor of size 6x5]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 11x10]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 11x10]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 11x13]\n",
      "\n",
      "\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      "[torch.FloatTensor of size 6x6]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 11x7]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 5x10]\n",
      "\n",
      "\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      "[torch.FloatTensor of size 6x5]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 11x9]\n",
      "\n",
      "\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      "[torch.FloatTensor of size 6x6]\n",
      "\n",
      "\n",
      "\n",
      "Columns 0 to 12 \n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     1     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "\n",
      "Columns 13 to 20 \n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 5x21]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 6x7]\n",
      "\n",
      "\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      "[torch.FloatTensor of size 6x6]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 11x9]\n",
      "\n",
      "\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      "[torch.FloatTensor of size 6x5]\n",
      "\n",
      "\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "[torch.FloatTensor of size 11x5]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 11x10]\n",
      "\n",
      "\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "    0     0     0     0     0\n",
      "[torch.FloatTensor of size 11x5]\n",
      "\n",
      "\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      "[torch.FloatTensor of size 6x5]\n",
      "\n",
      "\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      "[torch.FloatTensor of size 6x5]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 11x7]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 6x11]\n",
      "\n",
      "\n",
      " 0  0  0  0\n",
      " 0  0  0  0\n",
      " 0  0  0  0\n",
      " 0  0  0  0\n",
      " 0  0  0  0\n",
      " 0  0  0  0\n",
      "[torch.FloatTensor of size 6x4]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 6x7]\n",
      "\n",
      "\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      " 0  0  0  0  0  0\n",
      "[torch.FloatTensor of size 6x6]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 5x7]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 11x8]\n",
      "\n",
      "\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 11x9]\n",
      "\n",
      "\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      "[torch.FloatTensor of size 6x5]\n",
      "\n",
      "\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      " 0  0  0  0  0\n",
      "[torch.FloatTensor of size 6x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import Sampler\n",
    "\n",
    "class Example():\n",
    "    def __init__(self, x_str, y_str, input_vocab, output_vocab):\n",
    "        self.x_str = x_str\n",
    "        self.y_str = y_str\n",
    "        self.x_toks = x_str.split(' ')\n",
    "        self.y_toks = y_str.split(' ')\n",
    "        \n",
    "        self.input_vocab = input_vocab\n",
    "        self.output_vocab = output_vocab\n",
    "        self.x_inds = torch.LongTensor(input_vocab.sentence_to_indices(x_str))\n",
    "        self.y_inds = torch.LongTensor(output_vocab.sentence_to_indices(y_str))\n",
    "        \n",
    "        # for copying\n",
    "        self.y_in_x_inds = torch.FloatTensor(([[int(x_tok == y_tok) for x_tok in self.x_toks] for y_tok in self.y_toks])) \n",
    "\n",
    "# In order to use PyTorch's data loader\n",
    "class ReaderDataset(Dataset):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.examples[index]\n",
    "    \n",
    "train_exs = []\n",
    "for x,y in zip(train_x, train_y):\n",
    "    train_exs.append(Example(x, y, input_vocab, output_vocab))\n",
    "train_dataset = ReaderDataset(train_exs)\n",
    "\n",
    "test_exs = []\n",
    "for x,y in zip(test_x, test_y):\n",
    "    test_exs.append(Example(x, y, input_vocab, output_vocab))\n",
    "test_dataset = ReaderDataset(test_exs)\n",
    "\n",
    "for x in test_dataset:\n",
    "    print(x.y_in_x_inds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize individual examples and organize them into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\n",
      "    3     4     5  ...      0     0     0\n",
      "   39     1     4  ...      0     0     0\n",
      "   12     5    13  ...      0     0     0\n",
      "       ...          ⋱          ...       \n",
      "   12     5    13  ...      0     0     0\n",
      "   19     5    20  ...      0     0     0\n",
      "    3     4     5  ...      0     0     0\n",
      "[torch.LongTensor of size 100x19]\n",
      ", \n",
      "   10\n",
      "   29\n",
      "   46\n",
      "   69\n",
      "   89\n",
      "  107\n",
      "  124\n",
      "  141\n",
      "  164\n",
      "  179\n",
      "  200\n",
      "  220\n",
      "  238\n",
      "  261\n",
      "  270\n",
      "  293\n",
      "  317\n",
      "  330\n",
      "  348\n",
      "  369\n",
      "  391\n",
      "  404\n",
      "  425\n",
      "  448\n",
      "  468\n",
      "  489\n",
      "  498\n",
      "  521\n",
      "  538\n",
      "  557\n",
      "  576\n",
      "  594\n",
      "  621\n",
      "  633\n",
      "  656\n",
      "  669\n",
      "  690\n",
      "  714\n",
      "  734\n",
      "  754\n",
      "  772\n",
      "  789\n",
      "  807\n",
      "  826\n",
      "  841\n",
      "  860\n",
      "  881\n",
      "  911\n",
      "  917\n",
      "  939\n",
      "  959\n",
      "  979\n",
      "  995\n",
      " 1013\n",
      " 1031\n",
      " 1057\n",
      " 1069\n",
      " 1089\n",
      " 1115\n",
      " 1128\n",
      " 1150\n",
      " 1164\n",
      " 1183\n",
      " 1206\n",
      " 1220\n",
      " 1246\n",
      " 1264\n",
      " 1285\n",
      " 1302\n",
      " 1317\n",
      " 1341\n",
      " 1358\n",
      " 1375\n",
      " 1393\n",
      " 1414\n",
      " 1431\n",
      " 1452\n",
      " 1470\n",
      " 1490\n",
      " 1506\n",
      " 1528\n",
      " 1548\n",
      " 1564\n",
      " 1586\n",
      " 1601\n",
      " 1626\n",
      " 1644\n",
      " 1665\n",
      " 1682\n",
      " 1701\n",
      " 1715\n",
      " 1739\n",
      " 1752\n",
      " 1778\n",
      " 1800\n",
      " 1811\n",
      " 1832\n",
      " 1852\n",
      " 1866\n",
      " 1890\n",
      "[torch.LongTensor of size 100]\n",
      ", \n",
      "    0     0     0  ...      1     1     1\n",
      "    0     0     0  ...      1     1     1\n",
      "    0     0     0  ...      1     1     1\n",
      "       ...          ⋱          ...       \n",
      "    0     0     0  ...      1     1     1\n",
      "    0     0     0  ...      1     1     1\n",
      "    0     0     0  ...      1     1     1\n",
      "[torch.ByteTensor of size 100x19]\n",
      ", \n",
      "   14     4     5  ...      7     8     2\n",
      "   14     4     5  ...      7     8     2\n",
      "    3     4     5  ...      0     0     0\n",
      "       ...          ⋱          ...       \n",
      "   16     4     5  ...      0     0     0\n",
      "    3     4     5  ...      0     0     0\n",
      "    9     4     5  ...      0     0     0\n",
      "[torch.LongTensor of size 100x12]\n",
      ", \n",
      "    1     1     1  ...      1     1     1\n",
      "    1     1     1  ...      1     1     1\n",
      "    1     1     1  ...      0     0     0\n",
      "       ...          ⋱          ...       \n",
      "    1     1     1  ...      0     0     0\n",
      "    1     1     1  ...      0     0     0\n",
      "    1     1     1  ...      0     0     0\n",
      "[torch.ByteTensor of size 100x12]\n",
      ", \n",
      "( 0 ,.,.) = \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "     ...       ⋱       ...    \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "\n",
      "( 1 ,.,.) = \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "     ...       ⋱       ...    \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "\n",
      "( 2 ,.,.) = \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "     ...       ⋱       ...    \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "... \n",
      "\n",
      "(97 ,.,.) = \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "     ...       ⋱       ...    \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "\n",
      "(98 ,.,.) = \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "     ...       ⋱       ...    \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "\n",
      "(99 ,.,.) = \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "     ...       ⋱       ...    \n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "   0   0   0  ...    0   0   0\n",
      "[torch.FloatTensor of size 100x12x19]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# vectorize batch data\n",
    "def vectorize(batch):\n",
    "    max_input_length = max([ex.x_inds.size(0) for ex in batch])\n",
    "    x = torch.LongTensor(len(batch), max_input_length).zero_() # initialize to 0\n",
    "    x_mask = torch.ByteTensor(len(batch), max_input_length).fill_(1) # mask used in softmax\n",
    "    x_lens = torch.LongTensor(len(batch)).zero_()\n",
    "    for i, ex in enumerate(batch):\n",
    "        x[i, :ex.x_inds.size(0)].copy_(ex.x_inds)\n",
    "        x_mask[i, :ex.x_inds.size(0)].fill_(0)\n",
    "        ###CHANGE: x_lens store the last index of each sequence. i*max_input_length is added so that later we can use \n",
    "        ###torch.index_select to get the last hidden states from a 2D tensor (batch_size*max_input_length, embedding_dim)\n",
    "        x_lens[i] = i*max_input_length+ex.x_inds.size(0)-1 \n",
    "    \n",
    "    max_output_length = max([ex.y_inds.size(0) for ex in batch])\n",
    "    y = torch.LongTensor(len(batch), max_output_length).zero_()\n",
    "    y_mask = torch.ByteTensor(len(batch), max_output_length).zero_() # for masked_select\n",
    "    for i, ex in enumerate(batch):\n",
    "        y[i, :ex.y_inds.size(0)].copy_(ex.y_inds)\n",
    "        y_mask[i, :ex.y_inds.size(0)].fill_(1)\n",
    "    \n",
    "    # for copying\n",
    "    y_in_x_inds = torch.FloatTensor(len(batch), max_output_length, max_input_length).zero_()\n",
    "    for i, ex in enumerate(batch):\n",
    "        y_in_x_inds[i, :ex.y_in_x_inds.size(0), :ex.y_in_x_inds.size(1)].copy_(ex.y_in_x_inds)\n",
    "\n",
    "    return x, x_lens, x_mask, y, y_mask, y_in_x_inds\n",
    "\n",
    "train_sampler = torch.utils.data.sampler.RandomSampler(train_dataset)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=100, ## the batch_size can be tuned\n",
    "    sampler=train_sampler,\n",
    "    num_workers=1,\n",
    "    collate_fn=vectorize\n",
    ")\n",
    "\n",
    "test_sampler = torch.utils.data.sampler.SequentialSampler(test_dataset)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1, ## the batch_size can be tuned\n",
    "    sampler=test_sampler,\n",
    "    num_workers=1,\n",
    "    collate_fn=vectorize\n",
    ")\n",
    "\n",
    "for x in train_loader:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Build the seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack bidirectional LSTM\n",
    "class StackBRNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=1):\n",
    "        super(StackBRNN, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.rnns = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            input_dim = input_dim if i == 0 else hidden_dim * 2\n",
    "            self.rnns.append(nn.LSTM(input_dim, hidden_dim, bidirectional=True))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Transpose batch and sequence dims\n",
    "        x = x.transpose(0, 1) # (seq_len, batch_size, input_dim)\n",
    "\n",
    "        outputs = [x]\n",
    "        for i in range(self.num_layers):\n",
    "            rnn_input = outputs[-1]\n",
    "            rnn_output = self.rnns[i](rnn_input)[0]\n",
    "            outputs.append(rnn_output)\n",
    "\n",
    "        h_output = outputs[-1]\n",
    "\n",
    "        # Transpose back\n",
    "        h_output = h_output.transpose(0, 1) # (batch_size, seq_len, 2*hidden_dim)\n",
    "        \n",
    "        return h_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2.1: Define the basic seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, input_vocab, output_vocab, copying=False):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.input_vocab = input_vocab\n",
    "        self.output_vocab = output_vocab\n",
    "        self.in_vocab_size = len(self.input_vocab)\n",
    "        self.out_vocab_size = len(self.output_vocab)\n",
    "        \n",
    "        self.in_embedding = nn.Embedding(self.in_vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.encoder = StackBRNN(embedding_dim, hidden_dim)\n",
    "        \n",
    "        self.out_embedding = nn.Embedding(self.out_vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        #Inputs: input, (h_0, c_0)\n",
    "        #Outputs: h_1, c_1\n",
    "        self.decoder = nn.LSTMCell(embedding_dim, hidden_dim) \n",
    "         \n",
    "        self.enc_to_dec = nn.Linear(hidden_dim*2, hidden_dim) # project encoding outupt\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_dim, self.out_vocab_size)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x_emb = self.in_embedding(x)\n",
    "        output = self.encoder(x_emb) # output: (batch_size, seq_len, hidden_dim*2)\n",
    "        return output\n",
    "    \n",
    "    def decode(self, h_prev):\n",
    "        out = self.output_layer(h_prev[0])\n",
    "        probs = F.softmax(out, dim=1)\n",
    "        \n",
    "        return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"attention.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSeq2Seq(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, input_vocab, output_vocab, copying=False):\n",
    "        super(AttentionSeq2Seq, self).__init__()\n",
    "        self.input_vocab = input_vocab\n",
    "        self.output_vocab = output_vocab\n",
    "        self.in_vocab_size = len(self.input_vocab)\n",
    "        self.out_vocab_size = len(self.output_vocab)\n",
    "        self.copying = copying\n",
    "        \n",
    "        self.in_embedding = nn.Embedding(self.in_vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.encoder = StackBRNN(embedding_dim, hidden_dim)\n",
    "        \n",
    "        self.out_embedding = nn.Embedding(self.out_vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        #Inputs: input, (h_0, c_0)\n",
    "        #Outputs: h_1, c_1\n",
    "        self.decoder = nn.LSTMCell(embedding_dim + hidden_dim*2, hidden_dim) # concatenate y_t and context_t\n",
    "        \n",
    "        self.enc_to_dec = nn.Linear(hidden_dim*2, hidden_dim) # project encoding outupt\n",
    "        \n",
    "        self.output_layer = nn.Linear(hidden_dim + hidden_dim*2, self.out_vocab_size) # concatenate h_t and context_t\n",
    "        \n",
    "    def encode(self, x):\n",
    "        x_emb = self.in_embedding(x)\n",
    "        output = self.encoder(x_emb) # output: (batch_size, seq_len, hidden_dim*2)\n",
    "        return output\n",
    "    \n",
    "    def decode(self, encoder_outputs, encoder_proj_outputs, x_mask, h_prev):\n",
    "        # (batch_size, seq_len, hidden_dim) * (batch_size, hidden_dim, 1) - >(batch_size, seq_len, 1)\n",
    "        scores = torch.bmm(encoder_proj_outputs, h_prev[0].unsqueeze(2)).squeeze(2) # scores: (batch_size, seq_len)\n",
    "        scores.data.masked_fill_(x_mask.data, -float('inf'))\n",
    "        alpha = F.softmax(scores, dim=1)\n",
    "        # (batch_size, 1, seq_len) * (batch_size, seq_len, hidden_dim) - > (batch_size, 1, hidden_dim)\n",
    "        context_t = torch.bmm(alpha.unsqueeze(1), encoder_outputs).squeeze(1) # context_t: (batch_size, hidden_dim) \n",
    "        \n",
    "        out = self.output_layer(torch.cat([h_prev[0], context_t], 1))\n",
    "        \n",
    "        if self.copying: \n",
    "            probs = F.softmax(torch.cat([out, scores], 1), dim=1) # Appending scores over the input\n",
    "        else:\n",
    "            probs = F.softmax(out, dim=1)\n",
    "    \n",
    "        return probs, context_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2.2: Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize and train the network:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(ex, model, optim):\n",
    "    model.train()\n",
    "    \n",
    "    x, x_lens, x_mask, y, y_mask, y_in_x_inds = ex\n",
    "    \n",
    "    # Variable(x.cuda()) if using GPU\n",
    "    x, x_lens, x_mask, y, y_mask, y_in_x_inds = Variable(x), Variable(x_lens), Variable(x_mask), Variable(y), Variable(y_mask), Variable(y_in_x_inds)\n",
    "    \n",
    "    encoder_outputs = model.encode(x) # (batch_size, seq_len, hidden_dim*2)\n",
    "    encoder_proj_outputs = model.enc_to_dec(encoder_outputs) # (batch_size, seq_len, hidden_dim)\n",
    "    \n",
    "    ###CHANGE: make use of x_lens to index the last hidden states\n",
    "    batch_size = x.size(0)\n",
    "    seq_len = x.size(1)\n",
    "    h_0 = torch.index_select(encoder_proj_outputs.view(batch_size*seq_len,-1),0,x_lens) # be careful when input sequences have paddings\n",
    "    \n",
    "    c_0 = Variable(torch.zeros(h_0.size(0), h_0.size(1)).zero_()) \n",
    "    hidden = (h_0, c_0)\n",
    "    \n",
    "    p_y_seq = []\n",
    "    for i in range(y.size(1)):\n",
    "        #output = model.decode(hidden) \n",
    "        #y_emb = model.out_embedding(y[:, i]) # y_emb: (batch_size, embedding_dim)        \n",
    "        #hidden = model.decoder(y_emb, hidden) # (h_t, c_t): (batch_size, hidden_dim)\n",
    "        \n",
    "        ###CHANGE: update the decode function, move the code that uses y[:, i] down\n",
    "        output, context_t = model.decode(encoder_outputs, encoder_proj_outputs, x_mask, hidden) # with attention\n",
    "        \n",
    "        ###compute the next hidden state using the current output y[:, i]\n",
    "        y_emb = model.out_embedding(y[:, i]) # y_emb: (batch_size, embedding_dim)\n",
    "        hidden = model.decoder(torch.cat([y_emb, context_t], 1), hidden) \n",
    "        \n",
    "        p_y_t = output.gather(1, y[:, i].unsqueeze(1)) # (batch_size, 1)\n",
    "        \n",
    "        if model.copying:\n",
    "            copy_dist = output[:, model.out_vocab_size:model.out_vocab_size + y_in_x_inds.size(2)] # (batch_size, input_len)\n",
    "            # (batch_size, 1, input_len), (batch_size, input_len, 1)\n",
    "            copying_p_y_t = torch.bmm(copy_dist.unsqueeze(1), y_in_x_inds[:, i].unsqueeze(2)).squeeze(2)\n",
    "            p_y_t = p_y_t + copying_p_y_t\n",
    "                \n",
    "        p_y_seq.append(p_y_t)\n",
    "\n",
    "    p_y_seq = torch.cat([_ for _ in p_y_seq], 1) # (batch_size, seq_len)\n",
    "    p_y_seq = torch.masked_select(p_y_seq, y_mask)\n",
    "    loss = -torch.sum(torch.log(p_y_seq))/y.size(0) # loss = -\\sum_i log p(y|x)\n",
    "\n",
    "    # Clear gradients and run backward\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients, max_norm * v/||v|| if ||v|| > max_norm\n",
    "    torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=10.0)\n",
    "\n",
    "    # Update parameters\n",
    "    optim.step()\n",
    "    \n",
    "    return loss.data[0]\n",
    "\n",
    "#model = Seq2Seq(50, 20, input_vocab, output_vocab)\n",
    "model = AttentionSeq2Seq(50, 20, input_vocab, output_vocab, True)\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "# training loop\n",
    "n_epochs = 150\n",
    "for e in range(n_epochs):\n",
    "    train_loss = 0.0\n",
    "    for ex in train_loader:\n",
    "        l = train(ex, model, optim)\n",
    "        train_loss += l\n",
    "    print(\"Epoch = %d | Loss = %.2f\" % (e, train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2.3: Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model, similar to training. Using greedy search to infer the most likely sequence output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gold:  cor( WeatherHistroy [ 'Temperature' ] , WeatherHistory [ 'FellingTemperature' ] ) <end>\n",
      "Predict:  lm( WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'Temperature' ]\n",
      "Gold:  lm( WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'Humidity' ] ) <end>\n",
      "Predict:  lm( WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'Temperature' ]\n",
      "Gold:  lm( WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'Humidity' ] ) <end>\n",
      "Predict:  lm( WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'Temperature' ]\n",
      "Gold:  cor( WeatherHistroy [ 'Temperature' ] , WeatherHistory [ 'FellingTemperature' ] ) <end>\n",
      "Predict:  <end> WeatherHistory [ 'Temperature' ] ) <end> <end> <end> WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Gold:  max( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  mean( WeatherHistory [ 'Temperature' ] ) <end> <end> <end> 'Temperature' ] ) <end> <end> WeatherHistory\n",
      "Gold:  min( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  ) <end> WeatherHistory [ 'Temperature' ] ) <end> <end> <end> 'Temperature' ] ) <end> <end>\n",
      "Gold:  mean( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  mean( WeatherHistory [ 'Temperature' ] ) <end> <end> <end> 'Temperature' ] ) <end> <end> <end>\n",
      "Gold:  min( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  mean( WeatherHistory [ 'Temperature' ] ) <end> <end> <end> 'Temperature' ] ) <end> <end> <end>\n",
      "Gold:  predict( mod , 12 ) <end>\n",
      "Predict:  predict( mod , 12 ) <end> <end> <end> <end> <end> 12 ) <end> <end> <end>\n",
      "Gold:  cor( WeatherHistroy [ 'Temperature' ] , WeatherHistory [ 'FellingTemperature' ] ) <end>\n",
      "Predict:  lm( WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'Temperature' ]\n",
      "Gold:  mean( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  ) <end> WeatherHistory [ 'Temperature' ] ) <end> <end> <end> 'Temperature' ] ) <end> <end>\n",
      "Gold:  cor( WeatherHistroy [ 'Temperature' ] , WeatherHistory [ 'FellingTemperature' ] ) <end>\n",
      "Predict:  lm( WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'Temperature' ]\n",
      "Gold:  cor( WeatherHistroy [ 'Temperature' ] , WeatherHistory [ 'FellingTemperature' ] ) <end>\n",
      "Predict:  lm( WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'FellingTemperature' ] ) <end> <end> WeatherHistory [\n",
      "Gold:  cor( WeatherHistroy [ 'Temperature' ] , WeatherHistory [ 'FellingTemperature' ] ) <end>\n",
      "Predict:  lm( WeatherHistory [ 'Temperature' ] ) <end> <end> <end> WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Gold:  min( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  ) <end> WeatherHistory [ 'Temperature' ] ) <end> <end> <end> 'Temperature' ] ) <end> <end>\n",
      "Gold:  lm( WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'Humidity' ] ) <end>\n",
      "Predict:  lm( WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'Temperature' ]\n",
      "Gold:  predict( mod , 12 ) <end>\n",
      "Predict:  predict( mod , 12 ) <end> <end> <end> <end> <end> <end> <end> <end> <end> <end>\n",
      "Gold:  mean( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  ) <end> WeatherHistory [ 'Temperature' ] ) <end> <end> <end> 'Temperature' ] ) <end> <end>\n",
      "Gold:  cor( WeatherHistroy [ 'Temperature' ] , WeatherHistory [ 'FellingTemperature' ] ) <end>\n",
      "Predict:  lm( WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'FellingTemperature' ] ) <end> <end> WeatherHistory [\n",
      "Gold:  min( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  ) <end> WeatherHistory [ 'Temperature' ] ) <end> <end> <end> 'Temperature' ] ) <end> <end>\n",
      "Gold:  predict( mod , 12 ) <end>\n",
      "Predict:  predict( mod , 12 ) <end> <end> <end> WeatherHistory [ 'Temperature' ] , WeatherHistory [\n",
      "Gold:  min( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  max( WeatherHistory [ 'Temperature' ] ) <end> <end> <end> 'Temperature' ] ) <end> <end> <end>\n",
      "Gold:  min( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  ) <end> WeatherHistory [ 'Temperature' ] ) <end> <end> <end> 'Temperature' ] ) <end> <end>\n",
      "Gold:  cor( WeatherHistroy [ 'Temperature' ] , WeatherHistory [ 'FellingTemperature' ] ) <end>\n",
      "Predict:  lm( WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'Temperature' ]\n",
      "Gold:  mean( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  ) <end> WeatherHistory [ 'Temperature' ] ) <end> <end> <end> 'Temperature' ] ) <end> <end>\n",
      "Gold:  lm( WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'Humidity' ] ) <end>\n",
      "Predict:  lm( WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'Humidity' ] ) <end> <end> WeatherHistory [\n",
      "Gold:  lm( WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'Humidity' ] ) <end>\n",
      "Predict:  lm( WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'Humidity' ] ) <end> <end> WeatherHistory [\n",
      "Gold:  lm( WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'Humidity' ] ) <end>\n",
      "Predict:  lm( WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'Humidity' ] ) <end> <end> <end> WeatherHistory\n",
      "Gold:  max( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  ) <end> WeatherHistory [ 'Temperature' ] ) <end> <end> <end> 'Temperature' ] ) <end> <end>\n",
      "Gold:  min( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  lm( WeatherHistory [ 'Temperature' ] ) <end> <end> <end> WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Gold:  lm( WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'Humidity' ] ) <end>\n",
      "Predict:  lm( WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'Humidity' ] ) <end> <end> WeatherHistory [\n",
      "Gold:  max( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  mean( WeatherHistory [ 'Temperature' ] ) <end> <end> <end> 'Temperature' ] ) <end> <end> <end>\n",
      "Gold:  min( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  ) <end> WeatherHistory [ 'Temperature' ] ) <end> <end> <end> 'Temperature' ] ) <end> <end>\n",
      "Gold:  max( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  mean( WeatherHistory [ 'Temperature' ] ) <end> <end> <end> 'Temperature' ] ) <end> <end> <end>\n",
      "Gold:  min( WeatherHistory [ 'Temperature' ] ) <end>\n",
      "Predict:  ) <end> WeatherHistory [ 'Temperature' ] ) <end> <end> <end> 'Temperature' ] ) <end> <end>\n",
      "Gold:  predict( mod , 12 ) <end>\n",
      "Predict:  predict( mod , 12 ) <end> <end> <end> <end> <end> <end> <end> <end> <end> <end>\n",
      "Gold:  cor( WeatherHistroy [ 'Temperature' ] , WeatherHistory [ 'FellingTemperature' ] ) <end>\n",
      "Predict:  lm( WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'FellingTemperature' ] ) <end> <end> <end> WeatherHistory\n",
      "Gold:  lm( WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'Humidity' ] ) <end>\n",
      "Predict:  lm( WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'Temperature' ] , WeatherHistory [ 'Temperature' ]\n",
      "Gold:  min( WeatherHistory [ 'Humidity' ] ) <end>\n",
      "Predict:  ) <end> WeatherHistory [ 'Temperature' ] ) <end> <end> <end> 'Temperature' ] ) <end> <end>\n",
      "Gold:  min( WeatherHistory [ 'Humidity' ] ) <end>\n",
      "Predict:  ) <end> WeatherHistory [ 'Temperature' ] ) <end> <end> <end> 'Temperature' ] ) <end> <end>\n",
      "Test accuracy: 0.225\n"
     ]
    }
   ],
   "source": [
    "def test_batch(data_loader, model, max_len=15):\n",
    "    model.eval()\n",
    "    \n",
    "    num_correct = 0\n",
    "    for ex in data_loader:\n",
    "        x, x_lens, x_mask, y, y_mask, y_in_x_inds = ex \n",
    "        \n",
    "        x, x_lens, x_mask = Variable(x), Variable(x_lens), Variable(x_mask)\n",
    "    \n",
    "        encoder_outputs = model.encode(x) # (batch_size, seq_len, hidden_dim*2)\n",
    "        encoder_proj_outputs = model.enc_to_dec(encoder_outputs) # (batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        ###CHANGE: make use of x_lens to index the last hidden states\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "        h_0 = torch.index_select(encoder_proj_outputs.view(batch_size*seq_len,-1),0,x_lens) # be careful when input sequences have paddings\n",
    "    \n",
    "        c_0 = Variable(torch.zeros(h_0.size(0), h_0.size(1)).zero_()) \n",
    "        hidden = (h_0, c_0)\n",
    "        \n",
    "        ###CHANGE: start with empty prediction\n",
    "        seq = []\n",
    "        for i in range(max_len):\n",
    "            #output = model.decode(hidden) \n",
    "            \n",
    "            ###CHANGE: update the decode function, move the code that uses y[:, i] down\n",
    "            output, context_t = model.decode(encoder_outputs, encoder_proj_outputs, x_mask, hidden) # with attention\n",
    "        \n",
    "            sampleLogprobs, it = torch.max(output.data, 1)\n",
    "            y_t = it.view(-1).long()\n",
    "            seq.append(y_t)\n",
    "            \n",
    "            if model.copying:\n",
    "                new_y_t = []\n",
    "                for j in range(y_t.size(0)):\n",
    "                    if y_t[j] < model.out_vocab_size:\n",
    "                        new_y_t.append(y_t[j])\n",
    "                    else:\n",
    "                        k = x.data[j, y_t[j]-model.out_vocab_size]\n",
    "                        w = model.input_vocab.get_word(k)\n",
    "                        new_k = model.output_vocab.get_index(w)\n",
    "                        new_y_t.append(new_k)\n",
    "                y_t = torch.LongTensor(new_y_t)\n",
    "            \n",
    "            ###compute the next hidden state using the current output y_t\n",
    "            y_prev = Variable(y_t)\n",
    "            y_emb = model.out_embedding(y_prev) # y_emb: (batch_size, embedding_dim)\n",
    "            hidden = model.decoder(torch.cat([y_emb, context_t], 1), hidden) \n",
    "            \n",
    "            #hidden = model.decoder(y_emb, hidden)\n",
    "        \n",
    "        pred_y = torch.cat([_.unsqueeze(1) for _ in seq], 1)\n",
    "        \n",
    "        for idx in range(batch_size):\n",
    "            gold_toks = []\n",
    "            for wi in y[idx].tolist():\n",
    "                gold_toks.append(model.output_vocab.get_word(wi))\n",
    "            print(\"Gold: \", ' '.join(gold_toks))\n",
    "        \n",
    "            pred_toks = []\n",
    "            for wi in pred_y[idx].tolist():\n",
    "                #w = model.output_vocab.get_word(wi)\n",
    "            \n",
    "                if wi < model.out_vocab_size:\n",
    "                    w = model.output_vocab.get_word(wi)\n",
    "                else:\n",
    "                    w = model.input_vocab.get_word(x.data[idx][wi-model.out_vocab_size])\n",
    "                    #print(\"copying \", w)\n",
    "                    \n",
    "                pred_toks.append(w)\n",
    "                \n",
    "            print(\"Predict: \",' '.join(pred_toks))\n",
    "            \n",
    "            for i in range(len(gold_toks)):\n",
    "                g_tok = gold_toks[i]\n",
    "                p_tok = pred_toks[i]\n",
    "                if (g_tok != p_tok):\n",
    "                    break\n",
    "                elif (g_tok == \"<end>\"):\n",
    "                    num_correct += 1\n",
    "                    \n",
    "                    \n",
    "    print(\"Test accuracy: {}\".format(num_correct / len(data_loader)))\n",
    "                \n",
    "        \n",
    "test_batch(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
